{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bd799f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QML_Regression(Dataset, Features, Target):\n",
    "    \n",
    "    from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes, EfficientSU2\n",
    "    from qiskit_algorithms.optimizers import COBYLA, L_BFGS_B\n",
    "    from qiskit.primitives import Sampler\n",
    "    import time\n",
    "    from qiskit_machine_learning.algorithms.regressors import NeuralNetworkRegressor, VQR\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from qiskit_algorithms.utils import algorithm_globals\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from matplotlib import pyplot as plt\n",
    "    from IPython.display import clear_output\n",
    "    import numpy\n",
    "\n",
    "    \n",
    "    features = Features  # this need to be properly defined, if your data is not in data and label form split \n",
    "    labels = Target  # it into arrays change this features and label to make it two diff arrays\n",
    "    features = MinMaxScaler().fit_transform(features)\n",
    "    \n",
    "    algorithm_globals.random_seed = numpy.random.randint(1,100)\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        features, labels, train_size=0.7, random_state=algorithm_globals.random_seed\n",
    "    )\n",
    "\n",
    "    num_features = features.shape[1]\n",
    "\n",
    "    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
    "    feature_map.decompose()\n",
    "    \n",
    "    \n",
    "    ansatz = RealAmplitudes(num_qubits=num_features, reps=3)\n",
    "    ansatz.decompose()\n",
    "    \n",
    "    \n",
    "    optimizer = L_BFGS_B(maxiter=30)\n",
    "        \n",
    "    objective_func_vals = []\n",
    "    plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "    \n",
    "    def callback_graph(weights, obj_func):\n",
    "        clear_output(wait=True)\n",
    "        objective_func_vals.append(obj_func)\n",
    "        plt.title(\"Objective function value against iteration\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Objective function value\")\n",
    "        plt.plot(range(len(objective_func_vals)), objective_func_vals)\n",
    "        plt.show()\n",
    "        \n",
    "    vqr = VQR(\n",
    "    feature_map=feature_map,\n",
    "    ansatz=ansatz,\n",
    "    optimizer=optimizer,\n",
    "    callback=callback_graph,\n",
    "    )\n",
    "    \n",
    "    objective_func_vals = []\n",
    "    start = time.time()\n",
    "    vqr.fit(train_features, train_labels)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"Training time: {round(elapsed)} seconds\")\n",
    "    \n",
    "    train_score = vqr.score(train_features, train_labels)\n",
    "    test_score = vqr.score(test_features, test_labels)\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    \n",
    "    pred_train_features = vqr.predict(train_features)\n",
    "    pred_test_features = vqr.predict(test_features)\n",
    "    \n",
    "    values=[]\n",
    "    for i in range(len(test_features)):\n",
    "        values.append(i+2)\n",
    "        \n",
    "\n",
    "    train_mse = mean_squared_error(train_labels, pred_train_features)\n",
    "    test_mse = mean_squared_error(test_labels, pred_test_features)\n",
    "    train_r2 = r2_score(train_labels, pred_train_features)\n",
    "    test_r2 = r2_score(test_labels, pred_test_features)\n",
    "    \n",
    "    from sklearn import metrics\n",
    "    #confusion_matrix = metrics.confusion_matrix(train_labels, pred_train_features)\n",
    "    #cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)\n",
    "    \n",
    "    print(\"algorithm_globals.random seed =\", algorithm_globals.random_seed)\n",
    "    print(\"Train Score = \",train_score) \n",
    "    print(\"Test Score = \",test_score)\n",
    "    #print(\"Train Mean Squared Error = \",train_mse) \n",
    "    #print(\"Test Mean Squared Error = \",test_mse)\n",
    "    #print(\"Train r2 Score = \",train_r2) \n",
    "    #print(\"Test r2 Score = \",test_r2)\n",
    "\n",
    "    #cm_display.plot()\n",
    "    display(feature_map.decompose().draw('mpl'))\n",
    "    display(ansatz.decompose().draw('mpl'))\n",
    "    \n",
    "    plt.plot(values, test_labels, \"bo\") \n",
    "    plt.plot(values, pred_test_features, \"go\") \n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278f4dee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc36d629",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fef3144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>mainroad</th>\n",
       "      <th>guestroom</th>\n",
       "      <th>basement</th>\n",
       "      <th>hotwaterheating</th>\n",
       "      <th>airconditioning</th>\n",
       "      <th>parking</th>\n",
       "      <th>prefarea</th>\n",
       "      <th>furnishingstatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13300000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12250000</td>\n",
       "      <td>8960</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>no</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12250000</td>\n",
       "      <td>9960</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>semi-furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12215000</td>\n",
       "      <td>7500</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>3</td>\n",
       "      <td>yes</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11410000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>1820000</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>no</td>\n",
       "      <td>unfurnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>1767150</td>\n",
       "      <td>2400</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>semi-furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>1750000</td>\n",
       "      <td>3620</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>unfurnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>1750000</td>\n",
       "      <td>2910</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>furnished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>1750000</td>\n",
       "      <td>3850</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>0</td>\n",
       "      <td>no</td>\n",
       "      <td>unfurnished</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>545 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        price  area  bedrooms  bathrooms  stories mainroad guestroom basement  \\\n",
       "0    13300000  7420         4          2        3      yes        no       no   \n",
       "1    12250000  8960         4          4        4      yes        no       no   \n",
       "2    12250000  9960         3          2        2      yes        no      yes   \n",
       "3    12215000  7500         4          2        2      yes        no      yes   \n",
       "4    11410000  7420         4          1        2      yes       yes      yes   \n",
       "..        ...   ...       ...        ...      ...      ...       ...      ...   \n",
       "540   1820000  3000         2          1        1      yes        no      yes   \n",
       "541   1767150  2400         3          1        1       no        no       no   \n",
       "542   1750000  3620         2          1        1      yes        no       no   \n",
       "543   1750000  2910         3          1        1       no        no       no   \n",
       "544   1750000  3850         3          1        2      yes        no       no   \n",
       "\n",
       "    hotwaterheating airconditioning  parking prefarea furnishingstatus  \n",
       "0                no             yes        2      yes        furnished  \n",
       "1                no             yes        3       no        furnished  \n",
       "2                no              no        2      yes   semi-furnished  \n",
       "3                no             yes        3      yes        furnished  \n",
       "4                no             yes        2       no        furnished  \n",
       "..              ...             ...      ...      ...              ...  \n",
       "540              no              no        2       no      unfurnished  \n",
       "541              no              no        0       no   semi-furnished  \n",
       "542              no              no        0       no      unfurnished  \n",
       "543              no              no        0       no        furnished  \n",
       "544              no              no        0       no      unfurnished  \n",
       "\n",
       "[545 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house = pd.read_csv('Housing.csv')\n",
    "\"\"\"house = house.drop('guestroom', axis=1)\n",
    "house = house.drop('hotwaterheating', axis=1)\n",
    "house = house.drop('prefarea', axis=1)\n",
    "house = house.drop('basement', axis=1)\n",
    "house = house.drop('mainroad', axis=1)\n",
    "house = house.drop('parking', axis=1)\n",
    "house = house.drop('airconditioning', axis=1)\n",
    "house = house.drop('furnishingstatus', axis=1)\n",
    "house = house.drop('bathrooms', axis=1)\n",
    "\"\"\"\n",
    "house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aedb9b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>area</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>stories</th>\n",
       "      <th>mainroad</th>\n",
       "      <th>guestroom</th>\n",
       "      <th>basement</th>\n",
       "      <th>hotwaterheating</th>\n",
       "      <th>airconditioning</th>\n",
       "      <th>parking</th>\n",
       "      <th>prefarea</th>\n",
       "      <th>furnishingstatus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13300000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12250000</td>\n",
       "      <td>8960</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12250000</td>\n",
       "      <td>9960</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12215000</td>\n",
       "      <td>7500</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11410000</td>\n",
       "      <td>7420</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>1820000</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>541</th>\n",
       "      <td>1767150</td>\n",
       "      <td>2400</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>1750000</td>\n",
       "      <td>3620</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>543</th>\n",
       "      <td>1750000</td>\n",
       "      <td>2910</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>544</th>\n",
       "      <td>1750000</td>\n",
       "      <td>3850</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>545 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        price  area  bedrooms  bathrooms  stories  mainroad  guestroom  \\\n",
       "0    13300000  7420         4          2        3         1          0   \n",
       "1    12250000  8960         4          4        4         1          0   \n",
       "2    12250000  9960         3          2        2         1          0   \n",
       "3    12215000  7500         4          2        2         1          0   \n",
       "4    11410000  7420         4          1        2         1          1   \n",
       "..        ...   ...       ...        ...      ...       ...        ...   \n",
       "540   1820000  3000         2          1        1         1          0   \n",
       "541   1767150  2400         3          1        1         0          0   \n",
       "542   1750000  3620         2          1        1         1          0   \n",
       "543   1750000  2910         3          1        1         0          0   \n",
       "544   1750000  3850         3          1        2         1          0   \n",
       "\n",
       "     basement  hotwaterheating  airconditioning  parking  prefarea  \\\n",
       "0           0                0                1        2         1   \n",
       "1           0                0                1        3         0   \n",
       "2           1                0                0        2         1   \n",
       "3           1                0                1        3         1   \n",
       "4           1                0                1        2         0   \n",
       "..        ...              ...              ...      ...       ...   \n",
       "540         1                0                0        2         0   \n",
       "541         0                0                0        0         0   \n",
       "542         0                0                0        0         0   \n",
       "543         0                0                0        0         0   \n",
       "544         0                0                0        0         0   \n",
       "\n",
       "     furnishingstatus  \n",
       "0                   2  \n",
       "1                   2  \n",
       "2                   1  \n",
       "3                   2  \n",
       "4                   2  \n",
       "..                ...  \n",
       "540                 0  \n",
       "541                 1  \n",
       "542                 0  \n",
       "543                 2  \n",
       "544                 0  \n",
       "\n",
       "[545 rows x 13 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house['airconditioning'] = house['airconditioning'].map({'yes': 1, 'no': 0})\n",
    "house['mainroad'] = house['mainroad'].map({'yes': 1, 'no': 0})\n",
    "house['guestroom'] = house['guestroom'].map({'yes': 1, 'no': 0})\n",
    "house['basement'] = house['basement'].map({'yes': 1, 'no': 0})\n",
    "house['hotwaterheating'] = house['hotwaterheating'].map({'yes': 1, 'no': 0})\n",
    "house['prefarea'] = house['prefarea'].map({'yes': 1, 'no': 0})\n",
    "house['furnishingstatus'] = house['furnishingstatus'].map({'furnished': 2, 'unfurnished': 0, 'semi-furnished': 1})\n",
    "house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a872cbfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     price      area  bedrooms  bathrooms   stories  mainroad  \\\n",
      "price             1.000000  0.535997  0.366494   0.517545  0.420712  0.296898   \n",
      "area              0.535997  1.000000  0.151858   0.193820  0.083996  0.288874   \n",
      "bedrooms          0.366494  0.151858  1.000000   0.373930  0.408564 -0.012033   \n",
      "bathrooms         0.517545  0.193820  0.373930   1.000000  0.326165  0.042398   \n",
      "stories           0.420712  0.083996  0.408564   0.326165  1.000000  0.121706   \n",
      "mainroad          0.296898  0.288874 -0.012033   0.042398  0.121706  1.000000   \n",
      "guestroom         0.255517  0.140297  0.080549   0.126469  0.043538  0.092337   \n",
      "basement          0.187057  0.047417  0.097312   0.102106 -0.172394  0.044002   \n",
      "hotwaterheating   0.093073 -0.009229  0.046049   0.067159  0.018847 -0.011781   \n",
      "airconditioning   0.452954  0.222393  0.160603   0.186915  0.293602  0.105423   \n",
      "parking           0.384394  0.352980  0.139270   0.177496  0.045547  0.204433   \n",
      "prefarea          0.329777  0.234779  0.079023   0.063472  0.044425  0.199876   \n",
      "furnishingstatus  0.304721  0.171445  0.123244   0.143559  0.104672  0.156726   \n",
      "\n",
      "                  guestroom  basement  hotwaterheating  airconditioning  \\\n",
      "price              0.255517  0.187057         0.093073         0.452954   \n",
      "area               0.140297  0.047417        -0.009229         0.222393   \n",
      "bedrooms           0.080549  0.097312         0.046049         0.160603   \n",
      "bathrooms          0.126469  0.102106         0.067159         0.186915   \n",
      "stories            0.043538 -0.172394         0.018847         0.293602   \n",
      "mainroad           0.092337  0.044002        -0.011781         0.105423   \n",
      "guestroom          1.000000  0.372066        -0.010308         0.138179   \n",
      "basement           0.372066  1.000000         0.004385         0.047341   \n",
      "hotwaterheating   -0.010308  0.004385         1.000000        -0.130023   \n",
      "airconditioning    0.138179  0.047341        -0.130023         1.000000   \n",
      "parking            0.037466  0.051497         0.067864         0.159173   \n",
      "prefarea           0.160897  0.228083        -0.059411         0.117382   \n",
      "furnishingstatus   0.118328  0.112831         0.031628         0.150477   \n",
      "\n",
      "                   parking  prefarea  furnishingstatus  \n",
      "price             0.384394  0.329777          0.304721  \n",
      "area              0.352980  0.234779          0.171445  \n",
      "bedrooms          0.139270  0.079023          0.123244  \n",
      "bathrooms         0.177496  0.063472          0.143559  \n",
      "stories           0.045547  0.044425          0.104672  \n",
      "mainroad          0.204433  0.199876          0.156726  \n",
      "guestroom         0.037466  0.160897          0.118328  \n",
      "basement          0.051497  0.228083          0.112831  \n",
      "hotwaterheating   0.067864 -0.059411          0.031628  \n",
      "airconditioning   0.159173  0.117382          0.150477  \n",
      "parking           1.000000  0.091627          0.177539  \n",
      "prefarea          0.091627  1.000000          0.107686  \n",
      "furnishingstatus  0.177539  0.107686          1.000000  \n"
     ]
    }
   ],
   "source": [
    "data = house.iloc[:,[1,2,3,4,5,6,7,8,9,10,11,12]].values\n",
    "target_price = house.iloc[:, 0].values\n",
    "\n",
    "Dict = {'data' : data, 'target_price' : target_price}\n",
    "\n",
    "Features = Dict['data']\n",
    "\n",
    "Target = Dict['target_price']  # This is                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     only for Regression\n",
    "\n",
    "#Dict\n",
    "print(house.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb7aaaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55471ee6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'QML_Regression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m QML_Regression(Dict, Features, Target)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'QML_Regression' is not defined"
     ]
    }
   ],
   "source": [
    "QML_Regression(Dict, Features, Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2e6f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def QML_Reg(Dataset, Features, Target):\n",
    "    \n",
    "    from qiskit.circuit.library import ZZFeatureMap, RealAmplitudes, EfficientSU2\n",
    "    from qiskit_algorithms.optimizers import COBYLA, L_BFGS_B\n",
    "    from qiskit.primitives import Sampler\n",
    "    import time\n",
    "    from qiskit_machine_learning.algorithms.regressors import NeuralNetworkRegressor, VQR\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from qiskit_algorithms.utils import algorithm_globals\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from matplotlib import pyplot as plt\n",
    "    from IPython.display import clear_output\n",
    "    import numpy\n",
    "\n",
    "    \n",
    "    features = Features  # this need to be properly defined, if your data is not in data and label form split \n",
    "    labels = Target  # it into arrays change this features and label to make it two diff arrays\n",
    "    #features = MinMaxScaler().fit_transform(features)\n",
    "    \n",
    "    algorithm_globals.random_seed = numpy.random.randint(1,100)\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        features, labels, train_size=0.7, random_state=algorithm_globals.random_seed\n",
    "    )\n",
    "\n",
    "    num_features = features.shape[1]\n",
    "\n",
    "    feature_map = ZZFeatureMap(feature_dimension=num_features, reps=1)\n",
    "    feature_map.decompose()\n",
    "    \n",
    "    def Ansatz_housing(num_features):\n",
    "        ansatz_custom = QuantumCircuit(num_features)\n",
    "        j = 0\n",
    "        for i in range(num_features):\n",
    "            ansatz_custom.ry(Parameter('θ_' +str(j)), i)\n",
    "            j += 1\n",
    "            ansatz_custom.rz(Parameter('θ_' +str(j)), i)\n",
    "            j += 1\n",
    "        ansatz_custom.barrier()\n",
    "        \n",
    "        ansatz_custom.cx(0,1)\n",
    "        ansatz_custom.cx(0,3)\n",
    "        ansatz_custom.cx(0,4)\n",
    "        ansatz_custom.cx(0,9)\n",
    "        ansatz_custom.cx(0,10)\n",
    "        ansatz_custom.cx(2,4)\n",
    "        ansatz_custom.cx(2,3)\n",
    "        ansatz_custom.cx(7,6)\n",
    "        ansatz_custom.barrier()\n",
    "        \n",
    "        for i in range(num_features):\n",
    "            ansatz_custom.ry(Parameter('θ_' +str(j)), i)\n",
    "            j += 1\n",
    "            ansatz_custom.rz(Parameter('θ_' +str(j)), i)\n",
    "            j += 1\n",
    "        return ansatz_custom \n",
    "    \n",
    "    ansatz = Ansatz_housing(num_features) #RealAmplitudes(num_qubits=num_features, reps=2)\n",
    "    #ansatz.decompose()\n",
    "\n",
    "    \n",
    "    \n",
    "    optimizer = L_BFGS_B(maxiter=30)\n",
    "        \n",
    "    objective_func_vals = []\n",
    "    plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "    \n",
    "    def callback_graph(weights, obj_func):\n",
    "        clear_output(wait=True)\n",
    "        objective_func_vals.append(obj_func)\n",
    "        plt.title(\"Objective function value against iteration\")\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Objective function value\")\n",
    "        plt.plot(range(len(objective_func_vals)), objective_func_vals)\n",
    "        plt.show()\n",
    "        \n",
    "    vqr = VQR(\n",
    "    feature_map=feature_map,\n",
    "    ansatz=ansatz,\n",
    "    optimizer=optimizer,\n",
    "    callback=callback_graph,\n",
    "    )\n",
    "    \n",
    "    objective_func_vals = []\n",
    "    start = time.time()\n",
    "    vqr.fit(train_features, train_labels)\n",
    "    elapsed = time.time() - start\n",
    "    \n",
    "    print(f\"Training time: {round(elapsed)} seconds\")\n",
    "    \n",
    "    train_score = vqr.score(train_features, train_labels)\n",
    "    test_score = vqr.score(test_features, test_labels)\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    \n",
    "    pred_train_features = vqr.predict(train_features)\n",
    "    pred_test_features = vqr.predict(test_features)\n",
    "    \n",
    "    values=[]\n",
    "    for i in range(len(test_features)):\n",
    "        values.append(i+2)\n",
    "        \n",
    "\n",
    "    train_mse = mean_squared_error(train_labels, pred_train_features)\n",
    "    test_mse = mean_squared_error(test_labels, pred_test_features)\n",
    "    train_r2 = r2_score(train_labels, pred_train_features)\n",
    "    test_r2 = r2_score(test_labels, pred_test_features)\n",
    "    \n",
    "    from sklearn import metrics\n",
    "    #confusion_matrix = metrics.confusion_matrix(train_labels, pred_train_features)\n",
    "    #cm_display = metrics.ConfusionMatrixDisplay(confusion_matrix = confusion_matrix)\n",
    "    \n",
    "    print(\"algorithm_globals.random seed =\", algorithm_globals.random_seed)\n",
    "    print(\"Train Score = \",train_score) \n",
    "    print(\"Test Score = \",test_score)\n",
    "    #print(\"Train Mean Squared Error = \",train_mse) \n",
    "    #print(\"Test Mean Squared Error = \",test_mse)\n",
    "    #print(\"Train r2 Score = \",train_r2) \n",
    "    #print(\"Test r2 Score = \",test_r2)\n",
    "\n",
    "    #cm_display.plot()\n",
    "    display(feature_map.decompose().draw('mpl'))\n",
    "    display(ansatz.decompose().draw('mpl'))\n",
    "    \n",
    "    plt.plot(values, test_labels, \"bo\") \n",
    "    plt.plot(values, pred_test_features, \"go\") \n",
    "    \n",
    "    plt.show()\n",
    "    #print(vqr.predict([[5521, 4, 1],[4500, 2, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44c9c659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package qiskit_algorithms.optimizers in qiskit_algorithms:\n",
      "\n",
      "NAME\n",
      "    qiskit_algorithms.optimizers\n",
      "\n",
      "DESCRIPTION\n",
      "    Optimizers (:mod:`qiskit_algorithms.optimizers`)\n",
      "    ================================================\n",
      "    Classical Optimizers.\n",
      "    \n",
      "    This package contains a variety of classical optimizers and were designed for use by\n",
      "    qiskit_algorithm's quantum variational algorithms, such as :class:`~qiskit_algorithms.VQE`.\n",
      "    Logically, these optimizers can be divided into two categories:\n",
      "    \n",
      "    `Local Optimizers`_\n",
      "      Given an optimization problem, a **local optimizer** is a function\n",
      "      that attempts to find an optimal value within the neighboring set of a candidate solution.\n",
      "    \n",
      "    `Global Optimizers`_\n",
      "      Given an optimization problem, a **global optimizer** is a function\n",
      "      that attempts to find an optimal value among all possible solutions.\n",
      "    \n",
      "    .. currentmodule:: qiskit_algorithms.optimizers\n",
      "    \n",
      "    Optimizer Base Classes\n",
      "    ----------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: ../stubs/\n",
      "       :nosignatures:\n",
      "    \n",
      "       OptimizerResult\n",
      "       Optimizer\n",
      "       Minimizer\n",
      "    \n",
      "    Steppable Optimization\n",
      "    ----------------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: ../stubs/\n",
      "    \n",
      "       optimizer_utils\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: ../stubs/\n",
      "       :nosignatures:\n",
      "    \n",
      "       SteppableOptimizer\n",
      "       AskData\n",
      "       TellData\n",
      "       OptimizerState\n",
      "    \n",
      "    \n",
      "    Local Optimizers\n",
      "    ----------------\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: ../stubs/\n",
      "       :nosignatures:\n",
      "    \n",
      "       ADAM\n",
      "       AQGD\n",
      "       CG\n",
      "       COBYLA\n",
      "       L_BFGS_B\n",
      "       GSLS\n",
      "       GradientDescent\n",
      "       GradientDescentState\n",
      "       NELDER_MEAD\n",
      "       NFT\n",
      "       P_BFGS\n",
      "       POWELL\n",
      "       SLSQP\n",
      "       SPSA\n",
      "       QNSPSA\n",
      "       TNC\n",
      "       SciPyOptimizer\n",
      "       UMDA\n",
      "    \n",
      "    Qiskit also provides the following optimizers, which are built-out using the optimizers from\n",
      "    `scikit-quant <https://scikit-quant.readthedocs.io/en/latest/>`_. The ``scikit-quant`` package\n",
      "    is not installed by default but must be explicitly installed, if desired, by the user. The\n",
      "    optimizers therein are provided under various licenses, hence it has been made an optional install.\n",
      "    To install the ``scikit-quant`` dependent package you can use ``pip install scikit-quant``.\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: ../stubs/\n",
      "       :nosignatures:\n",
      "    \n",
      "       BOBYQA\n",
      "       IMFIL\n",
      "       SNOBFIT\n",
      "    \n",
      "    Global Optimizers\n",
      "    -----------------\n",
      "    The global optimizers here all use `NLOpt <https://nlopt.readthedocs.io/en/latest/>`_ for their\n",
      "    core function and can only be used if the optional dependent ``NLOpt`` package is installed.\n",
      "    To install the ``NLOpt`` dependent package you can use ``pip install nlopt``.\n",
      "    \n",
      "    .. autosummary::\n",
      "       :toctree: ../stubs/\n",
      "       :nosignatures:\n",
      "    \n",
      "       CRS\n",
      "       DIRECT_L\n",
      "       DIRECT_L_RAND\n",
      "       ESCH\n",
      "       ISRES\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    adam_amsgrad\n",
      "    aqgd\n",
      "    bobyqa\n",
      "    cg\n",
      "    cobyla\n",
      "    gradient_descent\n",
      "    gsls\n",
      "    imfil\n",
      "    l_bfgs_b\n",
      "    nelder_mead\n",
      "    nft\n",
      "    nlopts (package)\n",
      "    optimizer\n",
      "    optimizer_utils (package)\n",
      "    p_bfgs\n",
      "    powell\n",
      "    qnspsa\n",
      "    scipy_optimizer\n",
      "    slsqp\n",
      "    snobfit\n",
      "    spsa\n",
      "    steppable_optimizer\n",
      "    tnc\n",
      "    umda\n",
      "\n",
      "CLASSES\n",
      "    abc.ABC(builtins.object)\n",
      "        qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "            qiskit_algorithms.optimizers.adam_amsgrad.ADAM\n",
      "            qiskit_algorithms.optimizers.aqgd.AQGD\n",
      "            qiskit_algorithms.optimizers.bobyqa.BOBYQA\n",
      "            qiskit_algorithms.optimizers.gsls.GSLS\n",
      "            qiskit_algorithms.optimizers.imfil.IMFIL\n",
      "            qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer\n",
      "                qiskit_algorithms.optimizers.cg.CG\n",
      "                qiskit_algorithms.optimizers.cobyla.COBYLA\n",
      "                qiskit_algorithms.optimizers.l_bfgs_b.L_BFGS_B\n",
      "                qiskit_algorithms.optimizers.nelder_mead.NELDER_MEAD\n",
      "                qiskit_algorithms.optimizers.nft.NFT\n",
      "                qiskit_algorithms.optimizers.p_bfgs.P_BFGS\n",
      "                qiskit_algorithms.optimizers.powell.POWELL\n",
      "                qiskit_algorithms.optimizers.slsqp.SLSQP\n",
      "                qiskit_algorithms.optimizers.tnc.TNC\n",
      "            qiskit_algorithms.optimizers.snobfit.SNOBFIT\n",
      "            qiskit_algorithms.optimizers.spsa.SPSA\n",
      "                qiskit_algorithms.optimizers.qnspsa.QNSPSA\n",
      "            qiskit_algorithms.optimizers.steppable_optimizer.SteppableOptimizer\n",
      "                qiskit_algorithms.optimizers.gradient_descent.GradientDescent\n",
      "            qiskit_algorithms.optimizers.umda.UMDA\n",
      "        qiskit_algorithms.optimizers.steppable_optimizer.AskData\n",
      "        qiskit_algorithms.optimizers.steppable_optimizer.TellData\n",
      "    builtins.object\n",
      "        qiskit_algorithms.optimizers.steppable_optimizer.OptimizerState\n",
      "            qiskit_algorithms.optimizers.gradient_descent.GradientDescentState\n",
      "    enum.IntEnum(builtins.int, enum.ReprEnum)\n",
      "        qiskit_algorithms.optimizers.optimizer.OptimizerSupportLevel\n",
      "    qiskit_algorithms.algorithm_result.AlgorithmResult(abc.ABC)\n",
      "        qiskit_algorithms.optimizers.optimizer.OptimizerResult\n",
      "    qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer(qiskit_algorithms.optimizers.optimizer.Optimizer)\n",
      "        qiskit_algorithms.optimizers.nlopts.crs.CRS\n",
      "        qiskit_algorithms.optimizers.nlopts.direct_l.DIRECT_L\n",
      "        qiskit_algorithms.optimizers.nlopts.direct_l_rand.DIRECT_L_RAND\n",
      "        qiskit_algorithms.optimizers.nlopts.esch.ESCH\n",
      "        qiskit_algorithms.optimizers.nlopts.isres.ISRES\n",
      "    typing.Protocol(typing.Generic)\n",
      "        qiskit_algorithms.optimizers.optimizer.Minimizer\n",
      "    \n",
      "    class ADAM(qiskit_algorithms.optimizers.optimizer.Optimizer)\n",
      "     |  ADAM(maxiter: 'int' = 10000, tol: 'float' = 1e-06, lr: 'float' = 0.001, beta_1: 'float' = 0.9, beta_2: 'float' = 0.99, noise_factor: 'float' = 1e-08, eps: 'float' = 1e-10, amsgrad: 'bool' = False, snapshot_dir: 'str | None' = None) -> 'None'\n",
      "     |  \n",
      "     |  Adam and AMSGRAD optimizers.\n",
      "     |  \n",
      "     |  Adam [1] is a gradient-based optimization algorithm that is relies on adaptive estimates of\n",
      "     |  lower-order moments. The algorithm requires little memory and is invariant to diagonal\n",
      "     |  rescaling of the gradients. Furthermore, it is able to cope with non-stationary objective\n",
      "     |  functions and noisy and/or sparse gradients.\n",
      "     |  \n",
      "     |  AMSGRAD [2] (a variant of Adam) uses a 'long-term memory' of past gradients and, thereby,\n",
      "     |  improves convergence properties.\n",
      "     |  \n",
      "     |  References:\n",
      "     |  \n",
      "     |      [1]: Kingma, Diederik & Ba, Jimmy (2014), Adam: A Method for Stochastic Optimization.\n",
      "     |           `arXiv:1412.6980 <https://arxiv.org/abs/1412.6980>`_\n",
      "     |  \n",
      "     |      [2]: Sashank J. Reddi and Satyen Kale and Sanjiv Kumar (2018),\n",
      "     |           On the Convergence of Adam and Beyond.\n",
      "     |           `arXiv:1904.09237 <https://arxiv.org/abs/1904.09237>`_\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ADAM\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, maxiter: 'int' = 10000, tol: 'float' = 1e-06, lr: 'float' = 0.001, beta_1: 'float' = 0.9, beta_2: 'float' = 0.99, noise_factor: 'float' = 1e-08, eps: 'float' = 1e-10, amsgrad: 'bool' = False, snapshot_dir: 'str | None' = None) -> 'None'\n",
      "     |      Args:\n",
      "     |          maxiter: Maximum number of iterations\n",
      "     |          tol: Tolerance for termination\n",
      "     |          lr: Value >= 0, Learning rate.\n",
      "     |          beta_1: Value in range 0 to 1, Generally close to 1.\n",
      "     |          beta_2: Value in range 0 to 1, Generally close to 1.\n",
      "     |          noise_factor: Value >= 0, Noise factor\n",
      "     |          eps : Value >=0, Epsilon to be used for finite differences if no analytic\n",
      "     |              gradient method is given.\n",
      "     |          amsgrad: True to use AMSGRAD, False if not\n",
      "     |          snapshot_dir: If not None save the optimizer's parameter\n",
      "     |              after every step to the given directory\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Return support level dictionary\n",
      "     |  \n",
      "     |  load_params(self, load_dir: 'str') -> 'None'\n",
      "     |      Load iteration parameters for a file called ``adam_params.csv``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          load_dir: The directory containing ``adam_params.csv``.\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  save_params(self, snapshot_dir: 'str') -> 'None'\n",
      "     |      Save the current iteration parameters to a file called ``adam_params.csv``.\n",
      "     |      \n",
      "     |      Note:\n",
      "     |      \n",
      "     |          The current parameters are appended to the file, if it exists already.\n",
      "     |          The file is not overwritten.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          snapshot_dir: The directory to store the file in.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class AQGD(qiskit_algorithms.optimizers.optimizer.Optimizer)\n",
      "     |  AQGD(maxiter: 'int | list[int]' = 1000, eta: 'float | list[float]' = 1.0, tol: 'float' = 1e-06, momentum: 'float | list[float]' = 0.25, param_tol: 'float' = 1e-06, averaging: 'int' = 10) -> 'None'\n",
      "     |  \n",
      "     |  Analytic Quantum Gradient Descent (AQGD) with Epochs optimizer.\n",
      "     |  Performs gradient descent optimization with a momentum term, analytic gradients,\n",
      "     |  and customized step length schedule for parameterized quantum gates, i.e.\n",
      "     |  Pauli Rotations. See, for example:\n",
      "     |  \n",
      "     |  * K. Mitarai, M. Negoro, M. Kitagawa, and K. Fujii. (2018).\n",
      "     |    Quantum circuit learning. Phys. Rev. A 98, 032309.\n",
      "     |    https://arxiv.org/abs/1803.00745\n",
      "     |  \n",
      "     |  * Maria Schuld, Ville Bergholm, Christian Gogolin, Josh Izaac, Nathan Killoran. (2019).\n",
      "     |    Evaluating analytic gradients on quantum hardware. Phys. Rev. A 99, 032331.\n",
      "     |    https://arxiv.org/abs/1811.11184\n",
      "     |  \n",
      "     |  for further details on analytic gradients of parameterized quantum gates.\n",
      "     |  \n",
      "     |  Gradients are computed \"analytically\" using the quantum circuit when evaluating\n",
      "     |  the objective function.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AQGD\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, maxiter: 'int | list[int]' = 1000, eta: 'float | list[float]' = 1.0, tol: 'float' = 1e-06, momentum: 'float | list[float]' = 0.25, param_tol: 'float' = 1e-06, averaging: 'int' = 10) -> 'None'\n",
      "     |      Performs Analytical Quantum Gradient Descent (AQGD) with Epochs.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          maxiter: Maximum number of iterations (full gradient steps)\n",
      "     |          eta: The coefficient of the gradient update. Increasing this value\n",
      "     |              results in larger step sizes: param = previous_param - eta * deriv\n",
      "     |          tol: Tolerance for change in windowed average of objective values.\n",
      "     |              Convergence occurs when either objective tolerance is met OR parameter\n",
      "     |              tolerance is met.\n",
      "     |          momentum: Bias towards the previous gradient momentum in current\n",
      "     |              update. Must be within the bounds: [0,1)\n",
      "     |          param_tol: Tolerance for change in norm of parameters.\n",
      "     |          averaging: Length of window over which to average objective values for objective\n",
      "     |              convergence criterion\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          AlgorithmError: If the length of ``maxiter``, `momentum``, and ``eta`` is not the same.\n",
      "     |  \n",
      "     |  get_support_level(self) -> 'dict[str, OptimizerSupportLevel]'\n",
      "     |      Support level dictionary\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Dict[str, int]: gradient, bounds and initial point\n",
      "     |                          support information that is ignored/required.\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class AskData(abc.ABC)\n",
      "     |  AskData(x_fun: 'POINT | list[POINT] | None' = None, x_jac: 'POINT | list[POINT] | None' = None) -> None\n",
      "     |  \n",
      "     |  Base class for return type of :meth:`~.SteppableOptimizer.ask`.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      x_fun: Point or list of points where the function needs to be evaluated to compute the next\n",
      "     |      state of the optimizer.\n",
      "     |      x_jac: Point or list of points where the gradient/jacobian needs to be evaluated to compute\n",
      "     |      the next state of the optimizer.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      AskData\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, x_fun: 'POINT | list[POINT] | None' = None, x_jac: 'POINT | list[POINT] | None' = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'x_fun': 'POINT | list[POINT] | None', 'x_jac': 'PO...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'x_fun': Field(name='x_fun',type='POINT | list...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ('x_fun', 'x_jac')\n",
      "     |  \n",
      "     |  x_fun = None\n",
      "     |  \n",
      "     |  x_jac = None\n",
      "    \n",
      "    class BOBYQA(qiskit_algorithms.optimizers.optimizer.Optimizer)\n",
      "     |  BOBYQA(maxiter: 'int' = 1000) -> 'None'\n",
      "     |  \n",
      "     |  Bound Optimization BY Quadratic Approximation algorithm.\n",
      "     |  \n",
      "     |  BOBYQA finds local solutions to nonlinear, non-convex minimization problems with optional\n",
      "     |  bound constraints, without requirement of derivatives of the objective function.\n",
      "     |  \n",
      "     |  Uses skquant.opt installed with pip install scikit-quant.\n",
      "     |  For further detail, please refer to\n",
      "     |  https://github.com/scikit-quant/scikit-quant and https://qat4chem.lbl.gov/software.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BOBYQA\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, maxiter: 'int' = 1000) -> 'None'\n",
      "     |      Args:\n",
      "     |          maxiter: Maximum number of function evaluations.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          MissingOptionalLibraryError: scikit-quant not installed\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Returns support level dictionary.\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class CG(qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer)\n",
      "     |  CG(maxiter: 'int' = 20, disp: 'bool' = False, gtol: 'float' = 1e-05, tol: 'float | None' = None, eps: 'float' = 1.4901161193847656e-08, options: 'dict | None' = None, max_evals_grouped: 'int' = 1, **kwargs) -> 'None'\n",
      "     |  \n",
      "     |  Conjugate Gradient optimizer.\n",
      "     |  \n",
      "     |  CG is an algorithm for the numerical solution of systems of linear equations whose matrices are\n",
      "     |  symmetric and positive-definite. It is an *iterative algorithm* in that it uses an initial\n",
      "     |  guess to generate a sequence of improving approximate solutions for a problem,\n",
      "     |  in which each approximation is derived from the previous ones.  It is often used to solve\n",
      "     |  unconstrained optimization problems, such as energy minimization.\n",
      "     |  \n",
      "     |  Uses scipy.optimize.minimize CG.\n",
      "     |  For further detail, please refer to\n",
      "     |  https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CG\n",
      "     |      qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, maxiter: 'int' = 20, disp: 'bool' = False, gtol: 'float' = 1e-05, tol: 'float | None' = None, eps: 'float' = 1.4901161193847656e-08, options: 'dict | None' = None, max_evals_grouped: 'int' = 1, **kwargs) -> 'None'\n",
      "     |      Args:\n",
      "     |          maxiter: Maximum number of iterations to perform.\n",
      "     |          disp: Set to True to print convergence messages.\n",
      "     |          gtol: Gradient norm must be less than gtol before successful termination.\n",
      "     |          tol: Tolerance for termination.\n",
      "     |          eps: If jac is approximated, use this value for the step size.\n",
      "     |          options: A dictionary of solver options.\n",
      "     |          max_evals_grouped: Max number of default gradient evaluations performed simultaneously.\n",
      "     |          kwargs: additional kwargs for scipy.optimize.minimize.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer:\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Return support level dictionary\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class COBYLA(qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer)\n",
      "     |  COBYLA(maxiter: 'int' = 1000, disp: 'bool' = False, rhobeg: 'float' = 1.0, tol: 'float | None' = None, options: 'dict | None' = None, **kwargs) -> 'None'\n",
      "     |  \n",
      "     |  Constrained Optimization By Linear Approximation optimizer.\n",
      "     |  \n",
      "     |  COBYLA is a numerical optimization method for constrained problems\n",
      "     |  where the derivative of the objective function is not known.\n",
      "     |  \n",
      "     |  Uses scipy.optimize.minimize COBYLA.\n",
      "     |  For further detail, please refer to\n",
      "     |  https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      COBYLA\n",
      "     |      qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, maxiter: 'int' = 1000, disp: 'bool' = False, rhobeg: 'float' = 1.0, tol: 'float | None' = None, options: 'dict | None' = None, **kwargs) -> 'None'\n",
      "     |      Args:\n",
      "     |          maxiter: Maximum number of function evaluations.\n",
      "     |          disp: Set to True to print convergence messages.\n",
      "     |          rhobeg: Reasonable initial changes to the variables.\n",
      "     |          tol: Final accuracy in the optimization (not precisely guaranteed).\n",
      "     |               This is a lower bound on the size of the trust region.\n",
      "     |          options: A dictionary of solver options.\n",
      "     |          kwargs: additional kwargs for scipy.optimize.minimize.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer:\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Return support level dictionary\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class CRS(qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer)\n",
      "     |  CRS(max_evals: 'int' = 1000) -> 'None'\n",
      "     |  \n",
      "     |  Controlled Random Search (CRS) with local mutation optimizer.\n",
      "     |  \n",
      "     |  Controlled Random Search (CRS) with local mutation is part of the family of the CRS optimizers.\n",
      "     |  The CRS optimizers start with a random population of points, and randomly evolve these points\n",
      "     |  by heuristic rules. In the case of CRS with local mutation, the evolution is a randomized\n",
      "     |  version of the :class:`NELDER_MEAD` local optimizer.\n",
      "     |  \n",
      "     |  \n",
      "     |  NLopt global optimizer, derivative-free.\n",
      "     |  For further detail, please refer to\n",
      "     |  https://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#controlled-random-search-crs-with-local-mutation\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      CRS\n",
      "     |      qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  get_nlopt_optimizer(self) -> qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizerType\n",
      "     |      Return NLopt optimizer type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer:\n",
      "     |  \n",
      "     |  __init__(self, max_evals: 'int' = 1000) -> 'None'\n",
      "     |      Args:\n",
      "     |          max_evals: Maximum allowed number of function evaluations.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          MissingOptionalLibraryError: NLopt library not installed.\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      return support level dictionary\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DIRECT_L(qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer)\n",
      "     |  DIRECT_L(max_evals: 'int' = 1000) -> 'None'\n",
      "     |  \n",
      "     |  DIviding RECTangles Locally-biased optimizer.\n",
      "     |  \n",
      "     |  DIviding RECTangles (DIRECT) is a deterministic-search algorithms based on systematic division\n",
      "     |  of the search domain into increasingly smaller hyper-rectangles.\n",
      "     |  The DIRECT-L version is a \"locally biased\" variant of DIRECT that makes the algorithm more\n",
      "     |  biased towards local search, so that it is more efficient for functions with few local minima.\n",
      "     |  \n",
      "     |  NLopt global optimizer, derivative-free.\n",
      "     |  For further detail, please refer to\n",
      "     |  http://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#direct-and-direct-l\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DIRECT_L\n",
      "     |      qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  get_nlopt_optimizer(self) -> qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizerType\n",
      "     |      Return NLopt optimizer type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer:\n",
      "     |  \n",
      "     |  __init__(self, max_evals: 'int' = 1000) -> 'None'\n",
      "     |      Args:\n",
      "     |          max_evals: Maximum allowed number of function evaluations.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          MissingOptionalLibraryError: NLopt library not installed.\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      return support level dictionary\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DIRECT_L_RAND(qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer)\n",
      "     |  DIRECT_L_RAND(max_evals: 'int' = 1000) -> 'None'\n",
      "     |  \n",
      "     |  DIviding RECTangles Locally-biased Randomized optimizer.\n",
      "     |  \n",
      "     |  DIRECT-L RAND is the \"locally biased\" variant with some randomization in near-tie decisions.\n",
      "     |  See also :class:`DIRECT_L`\n",
      "     |  \n",
      "     |  NLopt global optimizer, derivative-free.\n",
      "     |  For further detail, please refer to\n",
      "     |  http://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#direct-and-direct-l\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DIRECT_L_RAND\n",
      "     |      qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  get_nlopt_optimizer(self) -> qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizerType\n",
      "     |      Return NLopt optimizer type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer:\n",
      "     |  \n",
      "     |  __init__(self, max_evals: 'int' = 1000) -> 'None'\n",
      "     |      Args:\n",
      "     |          max_evals: Maximum allowed number of function evaluations.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          MissingOptionalLibraryError: NLopt library not installed.\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      return support level dictionary\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ESCH(qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer)\n",
      "     |  ESCH(max_evals: 'int' = 1000) -> 'None'\n",
      "     |  \n",
      "     |  ESCH evolutionary optimizer.\n",
      "     |  \n",
      "     |  ESCH is an evolutionary algorithm for global optimization that supports bound constraints only.\n",
      "     |  Specifically, it does not support nonlinear constraints.\n",
      "     |  \n",
      "     |  NLopt global optimizer, derivative-free.\n",
      "     |  For further detail, please refer to\n",
      "     |  \n",
      "     |  http://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#esch-evolutionary-algorithm\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ESCH\n",
      "     |      qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  get_nlopt_optimizer(self) -> qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizerType\n",
      "     |      Return NLopt optimizer type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer:\n",
      "     |  \n",
      "     |  __init__(self, max_evals: 'int' = 1000) -> 'None'\n",
      "     |      Args:\n",
      "     |          max_evals: Maximum allowed number of function evaluations.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          MissingOptionalLibraryError: NLopt library not installed.\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      return support level dictionary\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class GSLS(qiskit_algorithms.optimizers.optimizer.Optimizer)\n",
      "     |  GSLS(maxiter: 'int' = 10000, max_eval: 'int' = 10000, disp: 'bool' = False, sampling_radius: 'float' = 1e-06, sample_size_factor: 'int' = 1, initial_step_size: 'float' = 0.01, min_step_size: 'float' = 1e-10, step_size_multiplier: 'float' = 0.4, armijo_parameter: 'float' = 0.1, min_gradient_norm: 'float' = 1e-08, max_failed_rejection_sampling: 'int' = 50) -> 'None'\n",
      "     |  \n",
      "     |  Gaussian-smoothed Line Search.\n",
      "     |  \n",
      "     |  An implementation of the line search algorithm described in\n",
      "     |  https://arxiv.org/pdf/1905.01332.pdf, using gradient approximation\n",
      "     |  based on Gaussian-smoothed samples on a sphere.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      This component has some function that is normally random. If you want to reproduce behavior\n",
      "     |      then you should set the random number generator seed in the algorithm_globals\n",
      "     |      (``qiskit_algorithms.utils.algorithm_globals.random_seed = seed``).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GSLS\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, maxiter: 'int' = 10000, max_eval: 'int' = 10000, disp: 'bool' = False, sampling_radius: 'float' = 1e-06, sample_size_factor: 'int' = 1, initial_step_size: 'float' = 0.01, min_step_size: 'float' = 1e-10, step_size_multiplier: 'float' = 0.4, armijo_parameter: 'float' = 0.1, min_gradient_norm: 'float' = 1e-08, max_failed_rejection_sampling: 'int' = 50) -> 'None'\n",
      "     |      Args:\n",
      "     |          maxiter: Maximum number of iterations.\n",
      "     |          max_eval: Maximum number of evaluations.\n",
      "     |          disp: Set to True to display convergence messages.\n",
      "     |          sampling_radius: Sampling radius to determine gradient estimate.\n",
      "     |          sample_size_factor: The size of the sample set at each iteration is this number\n",
      "     |              multiplied by the dimension of the problem, rounded to the nearest integer.\n",
      "     |          initial_step_size: Initial step size for the descent algorithm.\n",
      "     |          min_step_size: Minimum step size for the descent algorithm.\n",
      "     |          step_size_multiplier: Step size reduction after unsuccessful steps, in the\n",
      "     |              interval (0, 1).\n",
      "     |          armijo_parameter: Armijo parameter for sufficient decrease criterion, in the\n",
      "     |              interval (0, 1).\n",
      "     |          min_gradient_norm: If the gradient norm is below this threshold, the algorithm stops.\n",
      "     |          max_failed_rejection_sampling: Maximum number of attempts to sample points within\n",
      "     |              bounds.\n",
      "     |  \n",
      "     |  get_support_level(self) -> 'dict[str, int]'\n",
      "     |      Return support level dictionary.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A dictionary containing the support levels for different options.\n",
      "     |  \n",
      "     |  gradient_approximation(self, n: 'int', x: 'np.ndarray', x_value: 'float', directions: 'np.ndarray', sample_set_x: 'np.ndarray', sample_set_y: 'np.ndarray') -> 'np.ndarray'\n",
      "     |      Construct gradient approximation from given sample.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          n: Dimension of the problem.\n",
      "     |          x: Point around which the sample set was constructed.\n",
      "     |          x_value: Objective function value at x.\n",
      "     |          directions: Directions of the sample points wrt the central point x, as a 2D array.\n",
      "     |          sample_set_x: x-coordinates of the sample set, one point per row, as a 2D array.\n",
      "     |          sample_set_y: Objective function values of the points in sample_set_x, as a 1D array.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Gradient approximation at x, as a 1D array.\n",
      "     |  \n",
      "     |  ls_optimize(self, n: 'int', obj_fun: 'Callable[[POINT], float]', initial_point: 'np.ndarray', var_lb: 'np.ndarray', var_ub: 'np.ndarray') -> 'tuple[np.ndarray, float, int, float]'\n",
      "     |      Run the line search optimization.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          n: Dimension of the problem.\n",
      "     |          obj_fun: Objective function.\n",
      "     |          initial_point: Initial point.\n",
      "     |          var_lb: Vector of lower bounds on the decision variables. Vector elements can be -np.inf\n",
      "     |                  if the corresponding variable is unbounded from below.\n",
      "     |          var_ub: Vector of upper bounds on the decision variables. Vector elements can be np.inf\n",
      "     |                  if the corresponding variable is unbounded from below.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Final iterate as a vector, corresponding objective function value,\n",
      "     |          number of evaluations, and norm of the gradient estimate.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          ValueError: If the number of dimensions mismatches the size of the initial point or\n",
      "     |              the length of the lower or upper bound.\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  sample_points(self, n: 'int', x: 'np.ndarray', num_points: 'int') -> 'tuple[np.ndarray, np.ndarray]'\n",
      "     |      Sample ``num_points`` points around ``x`` on the ``n``-sphere of specified radius.\n",
      "     |      \n",
      "     |      The radius of the sphere is ``self._options['sampling_radius']``.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          n: Dimension of the problem.\n",
      "     |          x: Point around which the sample set is constructed.\n",
      "     |          num_points: Number of points in the sample set.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A tuple containing the sampling points and the directions.\n",
      "     |  \n",
      "     |  sample_set(self, n: 'int', x: 'np.ndarray', var_lb: 'np.ndarray', var_ub: 'np.ndarray', num_points: 'int') -> 'tuple[np.ndarray, np.ndarray]'\n",
      "     |      Construct sample set of given size.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          n: Dimension of the problem.\n",
      "     |          x: Point around which the sample set is constructed.\n",
      "     |          var_lb: Vector of lower bounds on the decision variables. Vector elements can be -np.inf\n",
      "     |              if the corresponding variable is unbounded from below.\n",
      "     |          var_ub: Vector of lower bounds on the decision variables. Vector elements can be np.inf\n",
      "     |              if the corresponding variable is unbounded from above.\n",
      "     |          num_points: Number of points in the sample set.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Matrices of (unit-norm) sample directions and sample points, one per row.\n",
      "     |          Both matrices are 2D arrays of floats.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          RuntimeError: If not enough samples could be generated within the bounds.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class GradientDescent(qiskit_algorithms.optimizers.steppable_optimizer.SteppableOptimizer)\n",
      "     |  GradientDescent(maxiter: 'int' = 100, learning_rate: 'float | list[float] | np.ndarray | Callable[[], Generator[float, None, None]]' = 0.01, tol: 'float' = 1e-07, callback: 'CALLBACK | None' = None, perturbation: 'float | None' = None) -> 'None'\n",
      "     |  \n",
      "     |  The gradient descent minimization routine.\n",
      "     |  \n",
      "     |  For a function :math:`f` and an initial point :math:`\\vec\\theta_0`, the standard (or \"vanilla\")\n",
      "     |  gradient descent method is an iterative scheme to find the minimum :math:`\\vec\\theta^*` of\n",
      "     |  :math:`f` by updating the parameters in the direction of the negative gradient of :math:`f`\n",
      "     |  \n",
      "     |  .. math::\n",
      "     |  \n",
      "     |      \\vec\\theta_{n+1} = \\vec\\theta_{n} - \\eta_n \\vec\\nabla f(\\vec\\theta_{n}),\n",
      "     |  \n",
      "     |  for a small learning rate :math:`\\eta_n > 0`.\n",
      "     |  \n",
      "     |  You can either provide the analytic gradient :math:`\\vec\\nabla f` as ``jac``\n",
      "     |  in the :meth:`~.minimize` method, or, if you do not provide it, use a finite difference\n",
      "     |  approximation of the gradient. To adapt the size of the perturbation in the finite difference\n",
      "     |  gradients, set the ``perturbation`` property in the initializer.\n",
      "     |  \n",
      "     |  This optimizer supports a callback function. If provided in the initializer, the optimizer\n",
      "     |  will call the callback in each iteration with the following information in this order:\n",
      "     |  current number of function values, current parameters, current function value, norm of current\n",
      "     |  gradient.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |      A minimum example that will use finite difference gradients with a default perturbation\n",
      "     |      of 0.01 and a default learning rate of 0.01.\n",
      "     |  \n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from qiskit_algorithms.optimizers import GradientDescent\n",
      "     |  \n",
      "     |          def f(x):\n",
      "     |              return (np.linalg.norm(x) - 1) ** 2\n",
      "     |  \n",
      "     |          initial_point = np.array([1, 0.5, -0.2])\n",
      "     |  \n",
      "     |          optimizer = GradientDescent(maxiter=100)\n",
      "     |  \n",
      "     |          result = optimizer.minimize(fun=fun, x0=initial_point)\n",
      "     |  \n",
      "     |          print(f\"Found minimum {result.x} at a value\"\n",
      "     |              \"of {result.fun} using {result.nfev} evaluations.\")\n",
      "     |  \n",
      "     |      An example where the learning rate is an iterator and we supply the analytic gradient.\n",
      "     |      Note how much faster this convergences (i.e. less ``nfev``) compared to the previous\n",
      "     |      example.\n",
      "     |  \n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from qiskit_algorithms.optimizers import GradientDescent\n",
      "     |  \n",
      "     |          def learning_rate():\n",
      "     |              power = 0.6\n",
      "     |              constant_coeff = 0.1\n",
      "     |              def power_law():\n",
      "     |                  n = 0\n",
      "     |                  while True:\n",
      "     |                      yield constant_coeff * (n ** power)\n",
      "     |                      n += 1\n",
      "     |  \n",
      "     |              return power_law()\n",
      "     |  \n",
      "     |          def f(x):\n",
      "     |              return (np.linalg.norm(x) - 1) ** 2\n",
      "     |  \n",
      "     |          def grad_f(x):\n",
      "     |              return 2 * (np.linalg.norm(x) - 1) * x / np.linalg.norm(x)\n",
      "     |  \n",
      "     |          initial_point = np.array([1, 0.5, -0.2])\n",
      "     |  \n",
      "     |          optimizer = GradientDescent(maxiter=100, learning_rate=learning_rate)\n",
      "     |          result = optimizer.minimize(fun=fun, jac=grad_f, x0=initial_point)\n",
      "     |  \n",
      "     |          print(f\"Found minimum {result.x} at a value\"\n",
      "     |          \"of {result.fun} using {result.nfev} evaluations.\")\n",
      "     |  \n",
      "     |  \n",
      "     |  An other example where the evaluation of the function has a chance of failing. The user, with\n",
      "     |  specific knowledge about his function can catch this errors and handle them before passing the\n",
      "     |  result to the optimizer.\n",
      "     |  \n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          import random\n",
      "     |          import numpy as np\n",
      "     |          from qiskit_algorithms.optimizers import GradientDescent\n",
      "     |  \n",
      "     |          def objective(x):\n",
      "     |              if random.choice([True, False]):\n",
      "     |                  return None\n",
      "     |              else:\n",
      "     |                  return (np.linalg.norm(x) - 1) ** 2\n",
      "     |  \n",
      "     |          def grad(x):\n",
      "     |              if random.choice([True, False]):\n",
      "     |                  return None\n",
      "     |              else:\n",
      "     |                  return 2 * (np.linalg.norm(x) - 1) * x / np.linalg.norm(x)\n",
      "     |  \n",
      "     |  \n",
      "     |          initial_point = np.random.normal(0, 1, size=(100,))\n",
      "     |  \n",
      "     |          optimizer = GradientDescent(maxiter=20)\n",
      "     |          optimizer.start(x0=initial_point, fun=objective, jac=grad)\n",
      "     |  \n",
      "     |          while optimizer.continue_condition():\n",
      "     |              ask_data = optimizer.ask()\n",
      "     |              evaluated_gradient = None\n",
      "     |  \n",
      "     |              while evaluated_gradient is None:\n",
      "     |                  evaluated_gradient = grad(ask_data.x_center)\n",
      "     |                  optimizer.state.njev += 1\n",
      "     |  \n",
      "     |              optimizer.state.nit += 1\n",
      "     |  \n",
      "     |              tell_data = TellData(eval_jac=evaluated_gradient)\n",
      "     |              optimizer.tell(ask_data=ask_data, tell_data=tell_data)\n",
      "     |  \n",
      "     |          result = optimizer.create_result()\n",
      "     |  \n",
      "     |  Users that aren't dealing with complicated functions and who are more familiar with step by step\n",
      "     |  optimization algorithms can use the :meth:`~.step` method which wraps the :meth:`~.ask`\n",
      "     |  and :meth:`~.tell` methods. In the same spirit the method :meth:`~.minimize` will optimize the\n",
      "     |  function and return the result.\n",
      "     |  \n",
      "     |  To see other libraries that use this interface one can visit:\n",
      "     |  https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/009_ask_and_tell.html\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GradientDescent\n",
      "     |      qiskit_algorithms.optimizers.steppable_optimizer.SteppableOptimizer\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, maxiter: 'int' = 100, learning_rate: 'float | list[float] | np.ndarray | Callable[[], Generator[float, None, None]]' = 0.01, tol: 'float' = 1e-07, callback: 'CALLBACK | None' = None, perturbation: 'float | None' = None) -> 'None'\n",
      "     |      Args:\n",
      "     |          maxiter: The maximum number of iterations.\n",
      "     |          learning_rate: A constant, list, array or factory of generators yielding learning rates\n",
      "     |                         for the parameter updates. See the docstring for an example.\n",
      "     |          tol: If the norm of the parameter update is smaller than this threshold, the\n",
      "     |              optimizer has converged.\n",
      "     |          perturbation: If no gradient is passed to :meth:`~.minimize` the gradient is\n",
      "     |              approximated with a forward finite difference scheme with ``perturbation``\n",
      "     |              perturbation in both directions (defaults to 1e-2 if required).\n",
      "     |              Ignored when we have an explicit function for the gradient.\n",
      "     |      Raises:\n",
      "     |          ValueError: If ``learning_rate`` is an array and its length is less than ``maxiter``.\n",
      "     |  \n",
      "     |  ask(self) -> 'AskData'\n",
      "     |      Returns an object with the data needed to evaluate the gradient.\n",
      "     |      \n",
      "     |      If this object contains a gradient function the gradient can be evaluated directly. Otherwise\n",
      "     |      approximate it with a finite difference scheme.\n",
      "     |  \n",
      "     |  continue_condition(self) -> 'bool'\n",
      "     |      Condition that indicates the optimization process should come to an end.\n",
      "     |      \n",
      "     |      When the stepsize is smaller than the tolerance, the optimization process is considered\n",
      "     |      finished.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``True`` if the optimization process should continue, ``False`` otherwise.\n",
      "     |  \n",
      "     |  create_result(self) -> 'OptimizerResult'\n",
      "     |      Creates a result of the optimization process.\n",
      "     |      \n",
      "     |      This result contains the best point, the best function value, the number of function/gradient\n",
      "     |      evaluations and the number of iterations.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization process.\n",
      "     |  \n",
      "     |  evaluate(self, ask_data: 'AskData') -> 'TellData'\n",
      "     |      Evaluates the gradient.\n",
      "     |      \n",
      "     |      It does so either by evaluating an analytic gradient or by approximating it with a\n",
      "     |      finite difference scheme. It will either add ``1`` to the number of gradient evaluations or add\n",
      "     |      ``N+1`` to the number of function evaluations (Where N is the dimension of the gradient).\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          ask_data: It contains the point where the gradient is to be evaluated and the gradient\n",
      "     |                    function or, in its absence, the objective function to perform a finite difference\n",
      "     |                    approximation.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The data containing the gradient evaluation.\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Get the support level dictionary.\n",
      "     |  \n",
      "     |  start(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'None'\n",
      "     |      Populates the state of the optimizer with the data provided and sets all the counters to 0.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: Function to minimize.\n",
      "     |          x0: Initial point.\n",
      "     |          jac: Function to compute the gradient.\n",
      "     |          bounds: Bounds of the search space.\n",
      "     |  \n",
      "     |  tell(self, ask_data: 'AskData', tell_data: 'TellData') -> 'None'\n",
      "     |      Updates :attr:`.~GradientDescentState.x` by an amount proportional to the learning\n",
      "     |      rate and value of the gradient at that point.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          ask_data: The data used to evaluate the function.\n",
      "     |          tell_data: The data from the function evaluation.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          ValueError: If the gradient passed doesn't have the right dimension.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  perturbation\n",
      "     |      Returns the perturbation.\n",
      "     |      \n",
      "     |      This is the perturbation used in the finite difference gradient approximation.\n",
      "     |  \n",
      "     |  state\n",
      "     |      Return the current state of the optimizer.\n",
      "     |  \n",
      "     |  tol\n",
      "     |      Returns the tolerance of the optimizer.\n",
      "     |      \n",
      "     |      Any step with smaller stepsize than this value will stop the optimization.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.steppable_optimizer.SteppableOptimizer:\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimizes the function.\n",
      "     |      \n",
      "     |      For well behaved functions the user can call this method to minimize a function.\n",
      "     |      If the user wants more control on how to evaluate the function a custom loop can be\n",
      "     |      created using :meth:`~.ask` and :meth:`~.tell` and evaluating the function manually.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: Function to minimize.\n",
      "     |          x0: Initial point.\n",
      "     |          jac: Function to compute the gradient.\n",
      "     |          bounds: Bounds of the search space.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Object containing the result of the optimization.\n",
      "     |  \n",
      "     |  step(self) -> 'None'\n",
      "     |      Performs one step in the optimization process.\n",
      "     |      \n",
      "     |      This method composes :meth:`~.ask`, :meth:`~.evaluate`, and :meth:`~.tell` to make a \"step\"\n",
      "     |      in the optimization process.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class GradientDescentState(qiskit_algorithms.optimizers.steppable_optimizer.OptimizerState)\n",
      "     |  GradientDescentState(x: 'POINT', fun: 'Callable[[POINT], float] | None', jac: 'Callable[[POINT], POINT] | None', nfev: 'int | None', njev: 'int | None', nit: 'int | None', stepsize: 'float | None', learning_rate: 'LearningRate') -> None\n",
      "     |  \n",
      "     |  State of :class:`~.GradientDescent`.\n",
      "     |  \n",
      "     |  Dataclass with all the information of an optimizer plus the learning_rate and the stepsize.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      GradientDescentState\n",
      "     |      qiskit_algorithms.optimizers.steppable_optimizer.OptimizerState\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, x: 'POINT', fun: 'Callable[[POINT], float] | None', jac: 'Callable[[POINT], POINT] | None', nfev: 'int | None', njev: 'int | None', nit: 'int | None', stepsize: 'float | None', learning_rate: 'LearningRate') -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'learning_rate': 'LearningRate', 'stepsize': 'float...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'fun': Field(name='fun',type='Callable[[POINT]...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ('x', 'fun', 'jac', 'nfev', 'njev', 'nit', 'stepsize'...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.steppable_optimizer.OptimizerState:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class IMFIL(qiskit_algorithms.optimizers.optimizer.Optimizer)\n",
      "     |  IMFIL(maxiter: 'int' = 1000) -> 'None'\n",
      "     |  \n",
      "     |  IMplicit FILtering algorithm.\n",
      "     |  \n",
      "     |  Implicit filtering is a way to solve bound-constrained optimization problems for\n",
      "     |  which derivatives are not available. In comparison to methods that use interpolation to\n",
      "     |  reconstruct the function and its higher derivatives, implicit filtering builds upon\n",
      "     |  coordinate search followed by interpolation to get an approximate gradient.\n",
      "     |  \n",
      "     |  Uses skquant.opt installed with pip install scikit-quant.\n",
      "     |  For further detail, please refer to\n",
      "     |  https://github.com/scikit-quant/scikit-quant and https://qat4chem.lbl.gov/software.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IMFIL\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, maxiter: 'int' = 1000) -> 'None'\n",
      "     |      Args:\n",
      "     |          maxiter: Maximum number of function evaluations.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          MissingOptionalLibraryError: scikit-quant not installed\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Returns support level dictionary.\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ISRES(qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer)\n",
      "     |  ISRES(max_evals: 'int' = 1000) -> 'None'\n",
      "     |  \n",
      "     |  Improved Stochastic Ranking Evolution Strategy optimizer.\n",
      "     |  \n",
      "     |  Improved Stochastic Ranking Evolution Strategy (ISRES) is an algorithm for\n",
      "     |  non-linearly constrained global optimization. It has heuristics to escape local optima,\n",
      "     |  even though convergence to a global optima is not guaranteed. The evolution strategy is based\n",
      "     |  on a combination of a mutation rule and differential variation. The fitness ranking is simply\n",
      "     |  via the objective function for problems without nonlinear constraints. When nonlinear\n",
      "     |  constraints are included, the `stochastic ranking proposed by Runarsson and Yao\n",
      "     |  <https://notendur.hi.is/tpr/software/sres/Tec311r.pdf>`__\n",
      "     |  is employed. This method supports arbitrary nonlinear inequality and equality constraints, in\n",
      "     |  addition to the bound constraints.\n",
      "     |  \n",
      "     |  NLopt global optimizer, derivative-free.\n",
      "     |  For further detail, please refer to\n",
      "     |  http://nlopt.readthedocs.io/en/latest/NLopt_Algorithms/#isres-improved-stochastic-ranking-evolution-strategy\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ISRES\n",
      "     |      qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  get_nlopt_optimizer(self) -> qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizerType\n",
      "     |      Return NLopt optimizer type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer:\n",
      "     |  \n",
      "     |  __init__(self, max_evals: 'int' = 1000) -> 'None'\n",
      "     |      Args:\n",
      "     |          max_evals: Maximum allowed number of function evaluations.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          MissingOptionalLibraryError: NLopt library not installed.\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      return support level dictionary\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.nlopts.nloptimizer.NLoptOptimizer:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class L_BFGS_B(qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer)\n",
      "     |  L_BFGS_B(maxfun: 'int' = 15000, maxiter: 'int' = 15000, ftol: 'SupportsFloat' = 2.220446049250313e-15, iprint: 'int' = -1, eps: 'float' = 1e-08, options: 'dict | None' = None, max_evals_grouped: 'int' = 1, **kwargs)\n",
      "     |  \n",
      "     |  Limited-memory BFGS Bound optimizer.\n",
      "     |  \n",
      "     |  The target goal of Limited-memory Broyden-Fletcher-Goldfarb-Shanno Bound (L-BFGS-B)\n",
      "     |  is to minimize the value of a differentiable scalar function :math:`f`.\n",
      "     |  This optimizer is a quasi-Newton method, meaning that, in contrast to Newtons's method,\n",
      "     |  it does not require :math:`f`'s Hessian (the matrix of :math:`f`'s second derivatives)\n",
      "     |  when attempting to compute :math:`f`'s minimum value.\n",
      "     |  \n",
      "     |  Like BFGS, L-BFGS is an iterative method for solving unconstrained, non-linear optimization\n",
      "     |  problems, but approximates BFGS using a limited amount of computer memory.\n",
      "     |  L-BFGS starts with an initial estimate of the optimal value, and proceeds iteratively\n",
      "     |  to refine that estimate with a sequence of better estimates.\n",
      "     |  \n",
      "     |  The derivatives of :math:`f` are used to identify the direction of steepest descent,\n",
      "     |  and also to form an estimate of the Hessian matrix (second derivative) of :math:`f`.\n",
      "     |  L-BFGS-B extends L-BFGS to handle simple, per-variable bound constraints.\n",
      "     |  \n",
      "     |  Uses ``scipy.optimize.fmin_l_bfgs_b``.\n",
      "     |  For further detail, please refer to\n",
      "     |  https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      L_BFGS_B\n",
      "     |      qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, maxfun: 'int' = 15000, maxiter: 'int' = 15000, ftol: 'SupportsFloat' = 2.220446049250313e-15, iprint: 'int' = -1, eps: 'float' = 1e-08, options: 'dict | None' = None, max_evals_grouped: 'int' = 1, **kwargs)\n",
      "     |      Args:\n",
      "     |          maxfun: Maximum number of function evaluations.\n",
      "     |          maxiter: Maximum number of iterations.\n",
      "     |          ftol: The iteration stops when\n",
      "     |              :math:`(f^k - f^{k+1}) / \\max\\{|f^k|, |f^{k+1}|,1\\} \\leq \\text{ftol}`.\n",
      "     |          iprint: Controls the frequency of output. ``iprint < 0`` means no output;\n",
      "     |              ``iprint = 0`` print only one line at the last iteration; ``0 < iprint < 99``\n",
      "     |              print also :math:`f` and :math:`|\\text{proj} g|` every iprint iterations;\n",
      "     |              ``iprint = 99`` print details of every iteration except n-vectors; ``iprint = 100``\n",
      "     |              print also the changes of active set and final :math:`x`; ``iprint > 100`` print\n",
      "     |              details of every iteration including :math:`x` and :math:`g`.\n",
      "     |          eps: If jac is approximated, use this value for the step size.\n",
      "     |          options: A dictionary of solver options.\n",
      "     |          max_evals_grouped: Max number of default gradient evaluations performed simultaneously.\n",
      "     |          kwargs: additional kwargs for ``scipy.optimize.minimize``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer:\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Return support level dictionary\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Minimizer(typing.Protocol)\n",
      "     |  Minimizer(*args, **kwargs)\n",
      "     |  \n",
      "     |  Callable Protocol for minimizer.\n",
      "     |  \n",
      "     |  This interface is based on `SciPy's optimize module\n",
      "     |  <https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html>`__.\n",
      "     |  \n",
      "     |   This protocol defines a callable taking the following parameters:\n",
      "     |  \n",
      "     |       fun\n",
      "     |           The objective function to minimize (for example the energy in the case of the VQE).\n",
      "     |       x0\n",
      "     |           The initial point for the optimization.\n",
      "     |       jac\n",
      "     |           The gradient of the objective function.\n",
      "     |       bounds\n",
      "     |           Parameters bounds for the optimization. Note that these might not be supported\n",
      "     |           by all optimizers.\n",
      "     |  \n",
      "     |   and which returns a minimization result object (either SciPy's or Qiskit's).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Minimizer\n",
      "     |      typing.Protocol\n",
      "     |      typing.Generic\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, fun: 'Callable[[np.ndarray], float]', x0: 'np.ndarray', jac: 'Callable[[np.ndarray], np.ndarray] | None', bounds: 'list[tuple[float, float]] | None') -> 'scipy.optimize.OptimizeResult | OptimizerResult'\n",
      "     |      Minimize the objective function.\n",
      "     |      \n",
      "     |      This interface is based on `SciPy's optimize module <https://docs.scipy.org/doc\n",
      "     |      /scipy/reference/generated/scipy.optimize.minimize.html>`__.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The objective function to minimize (for example the energy in the case of the VQE).\n",
      "     |          x0: The initial point for the optimization.\n",
      "     |          jac: The gradient of the objective function.\n",
      "     |          bounds: Parameters bounds for the optimization. Note that these might not be supported\n",
      "     |              by all optimizers.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |           The minimization result object (either SciPy's or Qiskit's).\n",
      "     |  \n",
      "     |  __init__ = _no_init_or_replace_init(self, *args, **kwargs)\n",
      "     |  \n",
      "     |  __subclasshook__ = _proto_hook(other)\n",
      "     |      # Set (or override) the protocol subclass hook.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  __parameters__ = ()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Protocol:\n",
      "     |  \n",
      "     |  __init_subclass__(*args, **kwargs) from typing._ProtocolMeta\n",
      "     |      This method is called when a class is subclassed.\n",
      "     |      \n",
      "     |      The default implementation does nothing. It may be\n",
      "     |      overridden to extend subclasses.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from typing.Generic:\n",
      "     |  \n",
      "     |  __class_getitem__(params) from typing._ProtocolMeta\n",
      "     |      Parameterizes a generic class.\n",
      "     |      \n",
      "     |      At least, parameterizing a generic class is the *main* thing this method\n",
      "     |      does. For example, for some generic class `Foo`, this is called when we\n",
      "     |      do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
      "     |      \n",
      "     |      However, note that this method is also called when defining generic\n",
      "     |      classes in the first place with `class Foo(Generic[T]): ...`.\n",
      "    \n",
      "    class NELDER_MEAD(qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer)\n",
      "     |  NELDER_MEAD(maxiter: 'int | None' = None, maxfev: 'int' = 1000, disp: 'bool' = False, xatol: 'float' = 0.0001, tol: 'float | None' = None, adaptive: 'bool' = False, options: 'dict | None' = None, **kwargs) -> 'None'\n",
      "     |  \n",
      "     |  Nelder-Mead optimizer.\n",
      "     |  \n",
      "     |  The Nelder-Mead algorithm performs unconstrained optimization; it ignores bounds\n",
      "     |  or constraints.  It is used to find the minimum or maximum of an objective function\n",
      "     |  in a multidimensional space.  It is based on the Simplex algorithm. Nelder-Mead\n",
      "     |  is robust in many applications, especially when the first and second derivatives of the\n",
      "     |  objective function are not known.\n",
      "     |  \n",
      "     |  However, if the numerical computation of the derivatives can be trusted to be accurate,\n",
      "     |  other algorithms using the first and/or second derivatives information might be preferred to\n",
      "     |  Nelder-Mead for their better performance in the general case, especially in consideration of\n",
      "     |  the fact that the Nelder–Mead technique is a heuristic search method that can converge to\n",
      "     |  non-stationary points.\n",
      "     |  \n",
      "     |  Uses scipy.optimize.minimize Nelder-Mead.\n",
      "     |  For further detail, please refer to\n",
      "     |  See https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NELDER_MEAD\n",
      "     |      qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, maxiter: 'int | None' = None, maxfev: 'int' = 1000, disp: 'bool' = False, xatol: 'float' = 0.0001, tol: 'float | None' = None, adaptive: 'bool' = False, options: 'dict | None' = None, **kwargs) -> 'None'\n",
      "     |      Args:\n",
      "     |          maxiter: Maximum allowed number of iterations. If both maxiter and maxfev are set,\n",
      "     |              minimization will stop at the first reached.\n",
      "     |          maxfev: Maximum allowed number of function evaluations. If both maxiter and\n",
      "     |              maxfev are set, minimization will stop at the first reached.\n",
      "     |          disp: Set to True to print convergence messages.\n",
      "     |          xatol: Absolute error in xopt between iterations that is acceptable for convergence.\n",
      "     |          tol: Tolerance for termination.\n",
      "     |          adaptive: Adapt algorithm parameters to dimensionality of problem.\n",
      "     |          options: A dictionary of solver options.\n",
      "     |          kwargs: additional kwargs for scipy.optimize.minimize.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer:\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Return support level dictionary\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class NFT(qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer)\n",
      "     |  NFT(maxiter: 'int | None' = None, maxfev: 'int' = 1024, disp: 'bool' = False, reset_interval: 'int' = 32, options: 'dict | None' = None, **kwargs) -> 'None'\n",
      "     |  \n",
      "     |  Nakanishi-Fujii-Todo algorithm.\n",
      "     |  \n",
      "     |  See https://arxiv.org/abs/1903.12166\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NFT\n",
      "     |      qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, maxiter: 'int | None' = None, maxfev: 'int' = 1024, disp: 'bool' = False, reset_interval: 'int' = 32, options: 'dict | None' = None, **kwargs) -> 'None'\n",
      "     |      Built out using scipy framework, for details, please refer to\n",
      "     |      https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          maxiter: Maximum number of iterations to perform.\n",
      "     |          maxfev: Maximum number of function evaluations to perform.\n",
      "     |          disp: disp\n",
      "     |          reset_interval: The minimum estimates directly once\n",
      "     |                          in ``reset_interval`` times.\n",
      "     |          options: A dictionary of solver options.\n",
      "     |          kwargs: additional kwargs for scipy.optimize.minimize.\n",
      "     |      \n",
      "     |      Notes:\n",
      "     |          In this optimization method, the optimization function have to satisfy\n",
      "     |          three conditions written in [1]_.\n",
      "     |      \n",
      "     |      References:\n",
      "     |          .. [1] K. M. Nakanishi, K. Fujii, and S. Todo. 2019.\n",
      "     |              Sequential minimal optimization for quantum-classical hybrid algorithms.\n",
      "     |              arXiv preprint arXiv:1903.12166.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer:\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Return support level dictionary\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class Optimizer(abc.ABC)\n",
      "     |  Base class for optimization algorithm.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self)\n",
      "     |      Initialize the optimization algorithm, setting the support\n",
      "     |      level for _gradient_support_level, _bound_support_level,\n",
      "     |      _initial_point_support_level, and empty options.\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Return support level dictionary\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__init__', 'get_support_level', 'min...\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "    \n",
      "    class OptimizerResult(qiskit_algorithms.algorithm_result.AlgorithmResult)\n",
      "     |  OptimizerResult() -> 'None'\n",
      "     |  \n",
      "     |  The result of an optimization routine.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OptimizerResult\n",
      "     |      qiskit_algorithms.algorithm_result.AlgorithmResult\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self) -> 'None'\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  fun\n",
      "     |      The final value of the minimization.\n",
      "     |  \n",
      "     |  jac\n",
      "     |      The final gradient of the minimization.\n",
      "     |  \n",
      "     |  nfev\n",
      "     |      The total number of function evaluations.\n",
      "     |  \n",
      "     |  nit\n",
      "     |      The total number of iterations.\n",
      "     |  \n",
      "     |  njev\n",
      "     |      The total number of gradient evaluations.\n",
      "     |  \n",
      "     |  x\n",
      "     |      The final point of the minimization.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.algorithm_result.AlgorithmResult:\n",
      "     |  \n",
      "     |  __str__(self) -> str\n",
      "     |      Return str(self).\n",
      "     |  \n",
      "     |  combine(self, result: 'AlgorithmResult') -> None\n",
      "     |      Any property from the argument that exists in the receiver is\n",
      "     |      updated.\n",
      "     |      Args:\n",
      "     |          result: Argument result with properties to be set.\n",
      "     |      Raises:\n",
      "     |          TypeError: Argument is None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.algorithm_result.AlgorithmResult:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class OptimizerState(builtins.object)\n",
      "     |  OptimizerState(x: 'POINT', fun: 'Callable[[POINT], float] | None', jac: 'Callable[[POINT], POINT] | None', nfev: 'int | None', njev: 'int | None', nit: 'int | None') -> None\n",
      "     |  \n",
      "     |  Base class representing the state of the optimizer.\n",
      "     |  \n",
      "     |  This class stores the current state of the optimizer, given by the current point and\n",
      "     |  (optionally) information like the function value, the gradient or the number of\n",
      "     |  function evaluations. This dataclass can also store any other individual variables that\n",
      "     |  change during the optimization.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, x: 'POINT', fun: 'Callable[[POINT], float] | None', jac: 'Callable[[POINT], POINT] | None', nfev: 'int | None', njev: 'int | None', nit: 'int | None') -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __annotations__ = {'fun': 'Callable[[POINT], float] | None', 'jac': 'C...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'fun': Field(name='fun',type='Callable[[POINT]...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ('x', 'fun', 'jac', 'nfev', 'njev', 'nit')\n",
      "    \n",
      "    class OptimizerSupportLevel(enum.IntEnum)\n",
      "     |  OptimizerSupportLevel(value, names=None, *, module=None, qualname=None, type=None, start=1, boundary=None)\n",
      "     |  \n",
      "     |  Support Level enum for features such as bounds, gradient and initial point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      OptimizerSupportLevel\n",
      "     |      enum.IntEnum\n",
      "     |      builtins.int\n",
      "     |      enum.ReprEnum\n",
      "     |      enum.Enum\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __format__(self, format_spec, /)\n",
      "     |      Default object formatter.\n",
      "     |  \n",
      "     |  __new__(cls, value)\n",
      "     |      Create and return a new object.  See help(type) for accurate signature.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  ignored = <OptimizerSupportLevel.ignored: 1>\n",
      "     |  \n",
      "     |  not_supported = <OptimizerSupportLevel.not_supported: 0>\n",
      "     |  \n",
      "     |  required = <OptimizerSupportLevel.required: 3>\n",
      "     |  \n",
      "     |  supported = <OptimizerSupportLevel.supported: 2>\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from enum.IntEnum:\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  __str__ = __repr__(self, /)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from builtins.int:\n",
      "     |  \n",
      "     |  __abs__(self, /)\n",
      "     |      abs(self)\n",
      "     |  \n",
      "     |  __add__(self, value, /)\n",
      "     |      Return self+value.\n",
      "     |  \n",
      "     |  __and__(self, value, /)\n",
      "     |      Return self&value.\n",
      "     |  \n",
      "     |  __bool__(self, /)\n",
      "     |      True if self else False\n",
      "     |  \n",
      "     |  __ceil__(...)\n",
      "     |      Ceiling of an Integral returns itself.\n",
      "     |  \n",
      "     |  __divmod__(self, value, /)\n",
      "     |      Return divmod(self, value).\n",
      "     |  \n",
      "     |  __eq__(self, value, /)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __float__(self, /)\n",
      "     |      float(self)\n",
      "     |  \n",
      "     |  __floor__(...)\n",
      "     |      Flooring an Integral returns itself.\n",
      "     |  \n",
      "     |  __floordiv__(self, value, /)\n",
      "     |      Return self//value.\n",
      "     |  \n",
      "     |  __ge__(self, value, /)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __getattribute__(self, name, /)\n",
      "     |      Return getattr(self, name).\n",
      "     |  \n",
      "     |  __getnewargs__(self, /)\n",
      "     |  \n",
      "     |  __gt__(self, value, /)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __hash__(self, /)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __index__(self, /)\n",
      "     |      Return self converted to an integer, if self is suitable for use as an index into a list.\n",
      "     |  \n",
      "     |  __int__(self, /)\n",
      "     |      int(self)\n",
      "     |  \n",
      "     |  __invert__(self, /)\n",
      "     |      ~self\n",
      "     |  \n",
      "     |  __le__(self, value, /)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lshift__(self, value, /)\n",
      "     |      Return self<<value.\n",
      "     |  \n",
      "     |  __lt__(self, value, /)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __mod__(self, value, /)\n",
      "     |      Return self%value.\n",
      "     |  \n",
      "     |  __mul__(self, value, /)\n",
      "     |      Return self*value.\n",
      "     |  \n",
      "     |  __ne__(self, value, /)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __neg__(self, /)\n",
      "     |      -self\n",
      "     |  \n",
      "     |  __or__(self, value, /)\n",
      "     |      Return self|value.\n",
      "     |  \n",
      "     |  __pos__(self, /)\n",
      "     |      +self\n",
      "     |  \n",
      "     |  __pow__(self, value, mod=None, /)\n",
      "     |      Return pow(self, value, mod).\n",
      "     |  \n",
      "     |  __radd__(self, value, /)\n",
      "     |      Return value+self.\n",
      "     |  \n",
      "     |  __rand__(self, value, /)\n",
      "     |      Return value&self.\n",
      "     |  \n",
      "     |  __rdivmod__(self, value, /)\n",
      "     |      Return divmod(value, self).\n",
      "     |  \n",
      "     |  __rfloordiv__(self, value, /)\n",
      "     |      Return value//self.\n",
      "     |  \n",
      "     |  __rlshift__(self, value, /)\n",
      "     |      Return value<<self.\n",
      "     |  \n",
      "     |  __rmod__(self, value, /)\n",
      "     |      Return value%self.\n",
      "     |  \n",
      "     |  __rmul__(self, value, /)\n",
      "     |      Return value*self.\n",
      "     |  \n",
      "     |  __ror__(self, value, /)\n",
      "     |      Return value|self.\n",
      "     |  \n",
      "     |  __round__(...)\n",
      "     |      Rounding an Integral returns itself.\n",
      "     |      \n",
      "     |      Rounding with an ndigits argument also returns an integer.\n",
      "     |  \n",
      "     |  __rpow__(self, value, mod=None, /)\n",
      "     |      Return pow(value, self, mod).\n",
      "     |  \n",
      "     |  __rrshift__(self, value, /)\n",
      "     |      Return value>>self.\n",
      "     |  \n",
      "     |  __rshift__(self, value, /)\n",
      "     |      Return self>>value.\n",
      "     |  \n",
      "     |  __rsub__(self, value, /)\n",
      "     |      Return value-self.\n",
      "     |  \n",
      "     |  __rtruediv__(self, value, /)\n",
      "     |      Return value/self.\n",
      "     |  \n",
      "     |  __rxor__(self, value, /)\n",
      "     |      Return value^self.\n",
      "     |  \n",
      "     |  __sizeof__(self, /)\n",
      "     |      Returns size in memory, in bytes.\n",
      "     |  \n",
      "     |  __sub__(self, value, /)\n",
      "     |      Return self-value.\n",
      "     |  \n",
      "     |  __truediv__(self, value, /)\n",
      "     |      Return self/value.\n",
      "     |  \n",
      "     |  __trunc__(...)\n",
      "     |      Truncating an Integral returns itself.\n",
      "     |  \n",
      "     |  __xor__(self, value, /)\n",
      "     |      Return self^value.\n",
      "     |  \n",
      "     |  as_integer_ratio(self, /)\n",
      "     |      Return integer ratio.\n",
      "     |      \n",
      "     |      Return a pair of integers, whose ratio is exactly equal to the original int\n",
      "     |      and with a positive denominator.\n",
      "     |      \n",
      "     |      >>> (10).as_integer_ratio()\n",
      "     |      (10, 1)\n",
      "     |      >>> (-10).as_integer_ratio()\n",
      "     |      (-10, 1)\n",
      "     |      >>> (0).as_integer_ratio()\n",
      "     |      (0, 1)\n",
      "     |  \n",
      "     |  bit_count(self, /)\n",
      "     |      Number of ones in the binary representation of the absolute value of self.\n",
      "     |      \n",
      "     |      Also known as the population count.\n",
      "     |      \n",
      "     |      >>> bin(13)\n",
      "     |      '0b1101'\n",
      "     |      >>> (13).bit_count()\n",
      "     |      3\n",
      "     |  \n",
      "     |  bit_length(self, /)\n",
      "     |      Number of bits necessary to represent self in binary.\n",
      "     |      \n",
      "     |      >>> bin(37)\n",
      "     |      '0b100101'\n",
      "     |      >>> (37).bit_length()\n",
      "     |      6\n",
      "     |  \n",
      "     |  conjugate(...)\n",
      "     |      Returns self, the complex conjugate of any int.\n",
      "     |  \n",
      "     |  to_bytes(self, /, length=1, byteorder='big', *, signed=False)\n",
      "     |      Return an array of bytes representing an integer.\n",
      "     |      \n",
      "     |      length\n",
      "     |        Length of bytes object to use.  An OverflowError is raised if the\n",
      "     |        integer is not representable with the given number of bytes.  Default\n",
      "     |        is length 1.\n",
      "     |      byteorder\n",
      "     |        The byte order used to represent the integer.  If byteorder is 'big',\n",
      "     |        the most significant byte is at the beginning of the byte array.  If\n",
      "     |        byteorder is 'little', the most significant byte is at the end of the\n",
      "     |        byte array.  To request the native byte order of the host system, use\n",
      "     |        `sys.byteorder' as the byte order value.  Default is to use 'big'.\n",
      "     |      signed\n",
      "     |        Determines whether two's complement is used to represent the integer.\n",
      "     |        If signed is False and a negative integer is given, an OverflowError\n",
      "     |        is raised.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from builtins.int:\n",
      "     |  \n",
      "     |  from_bytes(bytes, byteorder='big', *, signed=False) from enum.EnumType\n",
      "     |      Return the integer represented by the given array of bytes.\n",
      "     |      \n",
      "     |      bytes\n",
      "     |        Holds the array of bytes to convert.  The argument must either\n",
      "     |        support the buffer protocol or be an iterable object producing bytes.\n",
      "     |        Bytes and bytearray are examples of built-in objects that support the\n",
      "     |        buffer protocol.\n",
      "     |      byteorder\n",
      "     |        The byte order used to represent the integer.  If byteorder is 'big',\n",
      "     |        the most significant byte is at the beginning of the byte array.  If\n",
      "     |        byteorder is 'little', the most significant byte is at the end of the\n",
      "     |        byte array.  To request the native byte order of the host system, use\n",
      "     |        `sys.byteorder' as the byte order value.  Default is to use 'big'.\n",
      "     |      signed\n",
      "     |        Indicates whether two's complement is used to represent the integer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from builtins.int:\n",
      "     |  \n",
      "     |  denominator\n",
      "     |      the denominator of a rational number in lowest terms\n",
      "     |  \n",
      "     |  imag\n",
      "     |      the imaginary part of a complex number\n",
      "     |  \n",
      "     |  numerator\n",
      "     |      the numerator of a rational number in lowest terms\n",
      "     |  \n",
      "     |  real\n",
      "     |      the real part of a complex number\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from enum.Enum:\n",
      "     |  \n",
      "     |  __dir__(self)\n",
      "     |      Returns all members and all public methods\n",
      "     |  \n",
      "     |  __init__(self, *args, **kwds)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __reduce_ex__(self, proto)\n",
      "     |      Helper for pickle.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from enum.Enum:\n",
      "     |  \n",
      "     |  name\n",
      "     |      The name of the Enum member.\n",
      "     |  \n",
      "     |  value\n",
      "     |      The value of the Enum member.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from enum.EnumType:\n",
      "     |  \n",
      "     |  __contains__(member) from enum.EnumType\n",
      "     |      Return True if member is a member of this enum\n",
      "     |      raises TypeError if member is not an enum member\n",
      "     |      \n",
      "     |      note: in 3.12 TypeError will no longer be raised, and True will also be\n",
      "     |      returned if member is the value of a member in this enum\n",
      "     |  \n",
      "     |  __getitem__(name) from enum.EnumType\n",
      "     |      Return the member matching `name`.\n",
      "     |  \n",
      "     |  __iter__() from enum.EnumType\n",
      "     |      Return members in definition order.\n",
      "     |  \n",
      "     |  __len__() from enum.EnumType\n",
      "     |      Return the number of members (no aliases)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from enum.EnumType:\n",
      "     |  \n",
      "     |  __members__\n",
      "     |      Returns a mapping of member name->value.\n",
      "     |      \n",
      "     |      This mapping lists all enum members, including aliases. Note that this\n",
      "     |      is a read-only view of the internal mapping.\n",
      "    \n",
      "    class POWELL(qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer)\n",
      "     |  POWELL(maxiter: 'int | None' = None, maxfev: 'int' = 1000, disp: 'bool' = False, xtol: 'float' = 0.0001, tol: 'float | None' = None, options: 'dict | None' = None, **kwargs) -> 'None'\n",
      "     |  \n",
      "     |  Powell optimizer.\n",
      "     |  \n",
      "     |  The Powell algorithm performs unconstrained optimization; it ignores bounds or\n",
      "     |  constraints. Powell is a *conjugate direction method*: it performs sequential one-dimensional\n",
      "     |  minimization along each directional vector, which is updated at\n",
      "     |  each iteration of the main minimization loop. The function being minimized need not be\n",
      "     |  differentiable, and no derivatives are taken.\n",
      "     |  \n",
      "     |  Uses scipy.optimize.minimize Powell.\n",
      "     |  For further detail, please refer to\n",
      "     |  See https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      POWELL\n",
      "     |      qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, maxiter: 'int | None' = None, maxfev: 'int' = 1000, disp: 'bool' = False, xtol: 'float' = 0.0001, tol: 'float | None' = None, options: 'dict | None' = None, **kwargs) -> 'None'\n",
      "     |      Args:\n",
      "     |          maxiter: Maximum allowed number of iterations. If both maxiter and maxfev\n",
      "     |              are set, minimization will stop at the first reached.\n",
      "     |          maxfev: Maximum allowed number of function evaluations. If both maxiter and\n",
      "     |              maxfev are set, minimization will stop at the first reached.\n",
      "     |          disp: Set to True to print convergence messages.\n",
      "     |          xtol: Relative error in solution xopt acceptable for convergence.\n",
      "     |          tol: Tolerance for termination.\n",
      "     |          options: A dictionary of solver options.\n",
      "     |          kwargs: additional kwargs for scipy.optimize.minimize.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer:\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Return support level dictionary\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class P_BFGS(qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer)\n",
      "     |  P_BFGS(maxfun: 'int' = 1000, ftol: 'SupportsFloat' = 2.220446049250313e-15, iprint: 'int' = -1, max_processes: 'int | None' = None, options: 'dict | None' = None, max_evals_grouped: 'int' = 1, **kwargs) -> 'None'\n",
      "     |  \n",
      "     |  Parallelized Limited-memory BFGS optimizer.\n",
      "     |  \n",
      "     |  P-BFGS is a parallelized version of :class:`L_BFGS_B` with which it shares the same parameters.\n",
      "     |  P-BFGS can be useful when the target hardware is a quantum simulator running on a classical\n",
      "     |  machine. This allows the multiple processes to use simulation to potentially reach a minimum\n",
      "     |  faster. The parallelization may also help the optimizer avoid getting stuck at local optima.\n",
      "     |  \n",
      "     |  Uses scipy.optimize.fmin_l_bfgs_b.\n",
      "     |  For further detail, please refer to\n",
      "     |  https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      This component has some function that is normally random. If you want to reproduce behavior\n",
      "     |      then you should set the random number generator seed in the algorithm_globals\n",
      "     |      (``qiskit_algorithms.utils.algorithm_globals.random_seed = seed``).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      P_BFGS\n",
      "     |      qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, maxfun: 'int' = 1000, ftol: 'SupportsFloat' = 2.220446049250313e-15, iprint: 'int' = -1, max_processes: 'int | None' = None, options: 'dict | None' = None, max_evals_grouped: 'int' = 1, **kwargs) -> 'None'\n",
      "     |      Args:\n",
      "     |          maxfun: Maximum number of function evaluations.\n",
      "     |          ftol: The iteration stops when (f\\^k - f\\^{k+1})/max{\\|f\\^k\\|,\\|f\\^{k+1}\\|,1} <= ftol.\n",
      "     |          iprint: Controls the frequency of output. iprint < 0 means no output;\n",
      "     |              iprint = 0 print only one line at the last iteration; 0 < iprint < 99\n",
      "     |              print also f and \\|proj g\\| every iprint iterations; iprint = 99 print\n",
      "     |              details of every iteration except n-vectors; iprint = 100 print also the\n",
      "     |              changes of active set and final x; iprint > 100 print details of\n",
      "     |              every iteration including x and g.\n",
      "     |          max_processes: maximum number of processes allowed, has a min. value of 1 if not None.\n",
      "     |          options: A dictionary of solver options.\n",
      "     |          max_evals_grouped: Max number of default gradient evaluations performed simultaneously.\n",
      "     |          kwargs: additional kwargs for scipy.optimize.minimize.\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer:\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Return support level dictionary\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class QNSPSA(qiskit_algorithms.optimizers.spsa.SPSA)\n",
      "     |  QNSPSA(fidelity: 'FIDELITY', maxiter: 'int' = 100, blocking: 'bool' = True, allowed_increase: 'float | None' = None, learning_rate: 'float | Callable[[], Iterator] | None' = None, perturbation: 'float | Callable[[], Iterator] | None' = None, resamplings: 'int | dict[int, int]' = 1, perturbation_dims: 'int | None' = None, regularization: 'float | None' = None, hessian_delay: 'int' = 0, lse_solver: 'Callable[[np.ndarray, np.ndarray], np.ndarray] | None' = None, initial_hessian: 'np.ndarray | None' = None, callback: 'CALLBACK | None' = None, termination_checker: 'TERMINATIONCHECKER | None' = None) -> 'None'\n",
      "     |  \n",
      "     |  The Quantum Natural SPSA (QN-SPSA) optimizer.\n",
      "     |  \n",
      "     |  The QN-SPSA optimizer [1] is a stochastic optimizer that belongs to the family of gradient\n",
      "     |  descent methods. This optimizer is based on SPSA but attempts to improve the convergence by\n",
      "     |  sampling the **natural gradient** instead of the vanilla, first-order gradient. It achieves\n",
      "     |  this by approximating Hessian of the ``fidelity`` of the ansatz circuit.\n",
      "     |  \n",
      "     |  Compared to natural gradients, which require :math:`\\mathcal{O}(d^2)` expectation value\n",
      "     |  evaluations for a circuit with :math:`d` parameters, QN-SPSA only requires\n",
      "     |  :math:`\\mathcal{O}(1)` and can therefore significantly speed up the natural gradient calculation\n",
      "     |  by sacrificing some accuracy. Compared to SPSA, QN-SPSA requires 4 additional function\n",
      "     |  evaluations of the fidelity.\n",
      "     |  \n",
      "     |  The stochastic approximation of the natural gradient can be systematically improved by\n",
      "     |  increasing the number of ``resamplings``. This leads to a Monte Carlo-style convergence to\n",
      "     |  the exact, analytic value.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      This component has some function that is normally random. If you want to reproduce behavior\n",
      "     |      then you should set the random number generator seed in the algorithm_globals\n",
      "     |      (``qiskit_algorithms.utils.algorithm_globals.random_seed = seed``).\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |      This short example runs QN-SPSA for the ground state calculation of the ``Z ^ Z``\n",
      "     |      observable where the ansatz is a ``PauliTwoDesign`` circuit.\n",
      "     |  \n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          import numpy as np\n",
      "     |          from qiskit_algorithms.optimizers import QNSPSA\n",
      "     |          from qiskit.circuit.library import PauliTwoDesign\n",
      "     |          from qiskit.primitives import Estimator, Sampler\n",
      "     |          from qiskit.quantum_info import Pauli\n",
      "     |  \n",
      "     |          # problem setup\n",
      "     |          ansatz = PauliTwoDesign(2, reps=1, seed=2)\n",
      "     |          observable = Pauli(\"ZZ\")\n",
      "     |          initial_point = np.random.random(ansatz.num_parameters)\n",
      "     |  \n",
      "     |          # loss function\n",
      "     |          estimator = Estimator()\n",
      "     |  \n",
      "     |          def loss(x):\n",
      "     |              result = estimator.run([ansatz], [observable], [x]).result()\n",
      "     |              return np.real(result.values[0])\n",
      "     |  \n",
      "     |          # fidelity for estimation of the geometric tensor\n",
      "     |          sampler = Sampler()\n",
      "     |          fidelity = QNSPSA.get_fidelity(ansatz, sampler)\n",
      "     |  \n",
      "     |          # run QN-SPSA\n",
      "     |          qnspsa = QNSPSA(fidelity, maxiter=300)\n",
      "     |          result = qnspsa.optimize(ansatz.num_parameters, loss, initial_point=initial_point)\n",
      "     |  \n",
      "     |  References:\n",
      "     |  \n",
      "     |      [1] J. Gacon et al, \"Simultaneous Perturbation Stochastic Approximation of the Quantum\n",
      "     |      Fisher Information\", `arXiv:2103.09232 <https://arxiv.org/abs/2103.09232>`_\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      QNSPSA\n",
      "     |      qiskit_algorithms.optimizers.spsa.SPSA\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, fidelity: 'FIDELITY', maxiter: 'int' = 100, blocking: 'bool' = True, allowed_increase: 'float | None' = None, learning_rate: 'float | Callable[[], Iterator] | None' = None, perturbation: 'float | Callable[[], Iterator] | None' = None, resamplings: 'int | dict[int, int]' = 1, perturbation_dims: 'int | None' = None, regularization: 'float | None' = None, hessian_delay: 'int' = 0, lse_solver: 'Callable[[np.ndarray, np.ndarray], np.ndarray] | None' = None, initial_hessian: 'np.ndarray | None' = None, callback: 'CALLBACK | None' = None, termination_checker: 'TERMINATIONCHECKER | None' = None) -> 'None'\n",
      "     |      Args:\n",
      "     |          fidelity: A function to compute the fidelity of the ansatz state with itself for\n",
      "     |              two different sets of parameters.\n",
      "     |          maxiter: The maximum number of iterations. Note that this is not the maximal number\n",
      "     |              of function evaluations.\n",
      "     |          blocking: If True, only accepts updates that improve the loss (up to some allowed\n",
      "     |              increase, see next argument).\n",
      "     |          allowed_increase: If ``blocking`` is ``True``, this argument determines by how much\n",
      "     |              the loss can increase with the proposed parameters and still be accepted.\n",
      "     |              If ``None``, the allowed increases is calibrated automatically to be twice the\n",
      "     |              approximated standard deviation of the loss function.\n",
      "     |          learning_rate: The update step is the learning rate is multiplied with the gradient.\n",
      "     |              If the learning rate is a float, it remains constant over the course of the\n",
      "     |              optimization. It can also be a callable returning an iterator which yields the\n",
      "     |              learning rates for each optimization step.\n",
      "     |              If ``learning_rate`` is set ``perturbation`` must also be provided.\n",
      "     |          perturbation: Specifies the magnitude of the perturbation for the finite difference\n",
      "     |              approximation of the gradients. Can be either a float or a generator yielding\n",
      "     |              the perturbation magnitudes per step.\n",
      "     |              If ``perturbation`` is set ``learning_rate`` must also be provided.\n",
      "     |          resamplings: The number of times the gradient (and Hessian) is sampled using a random\n",
      "     |              direction to construct a gradient estimate. Per default the gradient is estimated\n",
      "     |              using only one random direction. If an integer, all iterations use the same number\n",
      "     |              of resamplings. If a dictionary, this is interpreted as\n",
      "     |              ``{iteration: number of resamplings per iteration}``.\n",
      "     |          perturbation_dims: The number of perturbed dimensions. Per default, all dimensions\n",
      "     |              are perturbed, but a smaller, fixed number can be perturbed. If set, the perturbed\n",
      "     |              dimensions are chosen uniformly at random.\n",
      "     |          regularization: To ensure the preconditioner is symmetric and positive definite, the\n",
      "     |              identity times a small coefficient is added to it. This generator yields that\n",
      "     |              coefficient.\n",
      "     |          hessian_delay: Start multiplying the gradient with the inverse Hessian only after a\n",
      "     |              certain number of iterations. The Hessian is still evaluated and therefore this\n",
      "     |              argument can be useful to first get a stable average over the last iterations before\n",
      "     |              using it as preconditioner.\n",
      "     |          lse_solver: The method to solve for the inverse of the Hessian. Per default an\n",
      "     |              exact LSE solver is used, but can e.g. be overwritten by a minimization routine.\n",
      "     |          initial_hessian: The initial guess for the Hessian. By default the identity matrix\n",
      "     |              is used.\n",
      "     |          callback: A callback function passed information in each iteration step. The\n",
      "     |              information is, in this order: the parameters, the function value, the number\n",
      "     |              of function evaluations, the stepsize, whether the step was accepted.\n",
      "     |          termination_checker: A callback function executed at the end of each iteration step. The\n",
      "     |              arguments are, in this order: the parameters, the function value, the number\n",
      "     |              of function evaluations, the stepsize, whether the step was accepted. If the callback\n",
      "     |              returns True, the optimization is terminated.\n",
      "     |              To prevent additional evaluations of the objective method, if the objective has not yet\n",
      "     |              been evaluated, the objective is estimated by taking the mean of the objective\n",
      "     |              evaluations used in the estimate of the gradient.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  get_fidelity(circuit: 'QuantumCircuit', *, sampler: 'BaseSampler | None' = None) -> 'Callable[[np.ndarray, np.ndarray], float]'\n",
      "     |      Get a function to compute the fidelity of ``circuit`` with itself.\n",
      "     |      \n",
      "     |      Let ``circuit`` be a parameterized quantum circuit performing the operation\n",
      "     |      :math:`U(\\theta)` given a set of parameters :math:`\\theta`. Then this method returns\n",
      "     |      a function to evaluate\n",
      "     |      \n",
      "     |      .. math::\n",
      "     |      \n",
      "     |          F(\\theta, \\phi) = \\big|\\langle 0 | U^\\dagger(\\theta) U(\\phi) |0\\rangle  \\big|^2.\n",
      "     |      \n",
      "     |      The output of this function can be used as input for the ``fidelity`` to the\n",
      "     |      :class:`~.QNSPSA` optimizer.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          circuit: The circuit preparing the parameterized ansatz.\n",
      "     |          sampler: A sampler primitive to sample from a quantum state.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          A handle to the function :math:`F`.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.spsa.SPSA:\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Get the support level dictionary.\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.spsa.SPSA:\n",
      "     |  \n",
      "     |  calibrate(loss: 'Callable[[np.ndarray], float]', initial_point: 'np.ndarray', c: 'float' = 0.2, stability_constant: 'float' = 0, target_magnitude: 'float | None' = None, alpha: 'float' = 0.602, gamma: 'float' = 0.101, modelspace: 'bool' = False, max_evals_grouped: 'int' = 1) -> 'tuple[Callable, Callable]'\n",
      "     |      Calibrate SPSA parameters with a power series as learning rate and perturbation coeffs.\n",
      "     |      \n",
      "     |      The power series are:\n",
      "     |      \n",
      "     |      .. math::\n",
      "     |      \n",
      "     |          a_k = \\frac{a}{(A + k + 1)^\\alpha}, c_k = \\frac{c}{(k + 1)^\\gamma}\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          loss: The loss function.\n",
      "     |          initial_point: The initial guess of the iteration.\n",
      "     |          c: The initial perturbation magnitude.\n",
      "     |          stability_constant: The value of `A`.\n",
      "     |          target_magnitude: The target magnitude for the first update step, defaults to\n",
      "     |              :math:`2\\pi / 10`.\n",
      "     |          alpha: The exponent of the learning rate power series.\n",
      "     |          gamma: The exponent of the perturbation power series.\n",
      "     |          modelspace: Whether the target magnitude is the difference of parameter values\n",
      "     |              or function values (= model space).\n",
      "     |          max_evals_grouped: The number of grouped evaluations supported by the loss function.\n",
      "     |              Defaults to 1, i.e. no grouping.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple(generator, generator): A tuple of power series generators, the first one for the\n",
      "     |              learning rate and the second one for the perturbation.\n",
      "     |  \n",
      "     |  estimate_stddev(loss: 'Callable[[np.ndarray], float]', initial_point: 'np.ndarray', avg: 'int' = 25, max_evals_grouped: 'int' = 1) -> 'float'\n",
      "     |      Estimate the standard deviation of the loss function.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SLSQP(qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer)\n",
      "     |  SLSQP(maxiter: 'int' = 100, disp: 'bool' = False, ftol: 'float' = 1e-06, tol: 'float | None' = None, eps: 'float' = 1.4901161193847656e-08, options: 'dict | None' = None, max_evals_grouped: 'int' = 1, **kwargs) -> 'None'\n",
      "     |  \n",
      "     |  Sequential Least SQuares Programming optimizer.\n",
      "     |  \n",
      "     |  SLSQP minimizes a function of several variables with any combination of bounds, equality\n",
      "     |  and inequality constraints. The method wraps the SLSQP Optimization subroutine originally\n",
      "     |  implemented by Dieter Kraft.\n",
      "     |  \n",
      "     |  SLSQP is ideal for mathematical problems for which the objective function and the constraints\n",
      "     |  are twice continuously differentiable. Note that the wrapper handles infinite values in bounds\n",
      "     |  by converting them into large floating values.\n",
      "     |  \n",
      "     |  Uses scipy.optimize.minimize SLSQP.\n",
      "     |  For further detail, please refer to\n",
      "     |  See https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SLSQP\n",
      "     |      qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, maxiter: 'int' = 100, disp: 'bool' = False, ftol: 'float' = 1e-06, tol: 'float | None' = None, eps: 'float' = 1.4901161193847656e-08, options: 'dict | None' = None, max_evals_grouped: 'int' = 1, **kwargs) -> 'None'\n",
      "     |      Args:\n",
      "     |          maxiter: Maximum number of iterations.\n",
      "     |          disp: Set to True to print convergence messages.\n",
      "     |          ftol: Precision goal for the value of f in the stopping criterion.\n",
      "     |          tol: Tolerance for termination.\n",
      "     |          eps: Step size used for numerical approximation of the Jacobian.\n",
      "     |          options: A dictionary of solver options.\n",
      "     |          max_evals_grouped: Max number of default gradient evaluations performed simultaneously.\n",
      "     |          kwargs: additional kwargs for scipy.optimize.minimize.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer:\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Return support level dictionary\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SNOBFIT(qiskit_algorithms.optimizers.optimizer.Optimizer)\n",
      "     |  SNOBFIT(maxiter: 'int' = 1000, maxfail: 'int' = 10, maxmp: 'int' = None, verbose: 'bool' = False) -> 'None'\n",
      "     |  \n",
      "     |  Stable Noisy Optimization by Branch and FIT algorithm.\n",
      "     |  \n",
      "     |  SnobFit is used for the optimization of derivative-free, noisy objective functions providing\n",
      "     |  robust and fast solutions of problems with continuous variables varying within bound.\n",
      "     |  \n",
      "     |  Uses skquant.opt installed with pip install scikit-quant.\n",
      "     |  For further detail, please refer to\n",
      "     |  https://github.com/scikit-quant/scikit-quant and https://qat4chem.lbl.gov/software.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SNOBFIT\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, maxiter: 'int' = 1000, maxfail: 'int' = 10, maxmp: 'int' = None, verbose: 'bool' = False) -> 'None'\n",
      "     |      Args:\n",
      "     |          maxiter: Maximum number of function evaluations.\n",
      "     |          maxmp: Maximum number of  model points requested for the local fit.\n",
      "     |               Default = 2 * number of parameters + 6 set to this value when None.\n",
      "     |          maxfail: Maximum number of failures to improve the solution. Stops the algorithm\n",
      "     |                  after maxfail is reached.\n",
      "     |          verbose: Provide verbose (debugging) output.\n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          MissingOptionalLibraryError: scikit-quant or SQSnobFit not installed\n",
      "     |          AlgorithmError: If NumPy 1.24.0 or above is installed.\n",
      "     |              See https://github.com/scikit-quant/scikit-quant/issues/24 for more details.\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Returns support level dictionary.\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SPSA(qiskit_algorithms.optimizers.optimizer.Optimizer)\n",
      "     |  SPSA(maxiter: 'int' = 100, blocking: 'bool' = False, allowed_increase: 'float | None' = None, trust_region: 'bool' = False, learning_rate: 'float | np.ndarray | Callable[[], Iterator] | None' = None, perturbation: 'float | np.ndarray | Callable[[], Iterator] | None' = None, last_avg: 'int' = 1, resamplings: 'int | dict[int, int]' = 1, perturbation_dims: 'int | None' = None, second_order: 'bool' = False, regularization: 'float | None' = None, hessian_delay: 'int' = 0, lse_solver: 'Callable[[np.ndarray, np.ndarray], np.ndarray] | None' = None, initial_hessian: 'np.ndarray | None' = None, callback: 'CALLBACK | None' = None, termination_checker: 'TERMINATIONCHECKER | None' = None) -> 'None'\n",
      "     |  \n",
      "     |  Simultaneous Perturbation Stochastic Approximation (SPSA) optimizer.\n",
      "     |  \n",
      "     |  SPSA [1] is an gradient descent method for optimizing systems with multiple unknown parameters.\n",
      "     |  As an optimization method, it is appropriately suited to large-scale population models,\n",
      "     |  adaptive modeling, and simulation optimization.\n",
      "     |  \n",
      "     |  .. seealso::\n",
      "     |  \n",
      "     |      Many examples are presented at the `SPSA Web site <http://www.jhuapl.edu/SPSA>`__.\n",
      "     |  \n",
      "     |  The main feature of SPSA is the stochastic gradient approximation, which requires only two\n",
      "     |  measurements of the objective function, regardless of the dimension of the optimization\n",
      "     |  problem.\n",
      "     |  \n",
      "     |  Additionally to standard, first-order SPSA, where only gradient information is used, this\n",
      "     |  implementation also allows second-order SPSA (2-SPSA) [2]. In 2-SPSA we also estimate the\n",
      "     |  Hessian of the loss with a stochastic approximation and multiply the gradient with the\n",
      "     |  inverse Hessian to take local curvature into account and improve convergence.\n",
      "     |  Notably this Hessian estimate requires only a constant number of function evaluations\n",
      "     |  unlike an exact evaluation of the Hessian, which scales quadratically in the number of\n",
      "     |  function evaluations.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      SPSA can be used in the presence of noise, and it is therefore indicated in situations\n",
      "     |      involving measurement uncertainty on a quantum computation when finding a minimum.\n",
      "     |      If you are executing a variational algorithm using a Quantum ASseMbly Language (QASM)\n",
      "     |      simulator or a real device, SPSA would be the most recommended choice among the optimizers\n",
      "     |      provided here.\n",
      "     |  \n",
      "     |  The optimization process can includes a calibration phase if neither the ``learning_rate`` nor\n",
      "     |  ``perturbation`` is provided, which requires additional functional evaluations.\n",
      "     |  (Note that either both or none must be set.) For further details on the automatic calibration,\n",
      "     |  please refer to the supplementary information section IV. of [3].\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      This component has some function that is normally random. If you want to reproduce behavior\n",
      "     |      then you should set the random number generator seed in the algorithm_globals\n",
      "     |      (``qiskit_algorithms.utils.algorithm_globals.random_seed = seed``).\n",
      "     |  \n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |      This short example runs SPSA for the ground state calculation of the ``Z ^ Z``\n",
      "     |      observable where the ansatz is a ``PauliTwoDesign`` circuit.\n",
      "     |  \n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          import numpy as np\n",
      "     |          from qiskit_algorithms.optimizers import SPSA\n",
      "     |          from qiskit.circuit.library import PauliTwoDesign\n",
      "     |          from qiskit.quantum_info import Pauli\n",
      "     |  \n",
      "     |          ansatz = PauliTwoDesign(2, reps=1, seed=2)\n",
      "     |          observable = Pauli(\"Z\") ^ Pauli(\"Z\")\n",
      "     |          initial_point = np.random.random(ansatz.num_parameters)\n",
      "     |  \n",
      "     |          def loss(x):\n",
      "     |              bound = ansatz.bind_parameters(x)\n",
      "     |              return np.real((StateFn(observable, is_measurement=True) @ StateFn(bound)).eval())\n",
      "     |  \n",
      "     |          spsa = SPSA(maxiter=300)\n",
      "     |          result = spsa.optimize(ansatz.num_parameters, loss, initial_point=initial_point)\n",
      "     |  \n",
      "     |      To use the Hessian information, i.e. 2-SPSA, you can add `second_order=True` to the\n",
      "     |      initializer of the `SPSA` class, the rest of the code remains the same.\n",
      "     |  \n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          two_spsa = SPSA(maxiter=300, second_order=True)\n",
      "     |          result = two_spsa.optimize(ansatz.num_parameters, loss, initial_point=initial_point)\n",
      "     |  \n",
      "     |      The `termination_checker` can be used to implement a custom termination criterion.\n",
      "     |  \n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          import numpy as np\n",
      "     |          from qiskit_algorithms.optimizers import SPSA\n",
      "     |  \n",
      "     |          def objective(x):\n",
      "     |              return np.linalg.norm(x) + .04*np.random.rand(1)\n",
      "     |  \n",
      "     |          class TerminationChecker:\n",
      "     |  \n",
      "     |              def __init__(self, N : int):\n",
      "     |                  self.N = N\n",
      "     |                  self.values = []\n",
      "     |  \n",
      "     |              def __call__(self, nfev, parameters, value, stepsize, accepted) -> bool:\n",
      "     |                  self.values.append(value)\n",
      "     |  \n",
      "     |                  if len(self.values) > self.N:\n",
      "     |                      last_values = self.values[-self.N:]\n",
      "     |                      pp = np.polyfit(range(self.N), last_values, 1)\n",
      "     |                      slope = pp[0] / self.N\n",
      "     |  \n",
      "     |                      if slope > 0:\n",
      "     |                          return True\n",
      "     |                  return False\n",
      "     |  \n",
      "     |          spsa = SPSA(maxiter=200, termination_checker=TerminationChecker(10))\n",
      "     |          parameters, value, niter = spsa.optimize(2, objective, initial_point=[0.5, 0.5])\n",
      "     |          print(f'SPSA completed after {niter} iterations')\n",
      "     |  \n",
      "     |  \n",
      "     |  References:\n",
      "     |  \n",
      "     |      [1]: J. C. Spall (1998). An Overview of the Simultaneous Perturbation Method for Efficient\n",
      "     |      Optimization, Johns Hopkins APL Technical Digest, 19(4), 482–492.\n",
      "     |      `Online at jhuapl.edu. <https://www.jhuapl.edu/SPSA/PDF-SPSA/Spall_An_Overview.PDF>`_\n",
      "     |  \n",
      "     |      [2]: J. C. Spall (1997). Accelerated second-order stochastic optimization using only\n",
      "     |      function measurements, Proceedings of the 36th IEEE Conference on Decision and Control,\n",
      "     |      1417-1424 vol.2. `Online at IEEE.org. <https://ieeexplore.ieee.org/document/657661>`_\n",
      "     |  \n",
      "     |      [3]: A. Kandala et al. (2017). Hardware-efficient Variational Quantum Eigensolver for\n",
      "     |      Small Molecules and Quantum Magnets. Nature 549, pages242–246(2017).\n",
      "     |      `arXiv:1704.05018v2 <https://arxiv.org/pdf/1704.05018v2.pdf#section*.11>`_\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SPSA\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, maxiter: 'int' = 100, blocking: 'bool' = False, allowed_increase: 'float | None' = None, trust_region: 'bool' = False, learning_rate: 'float | np.ndarray | Callable[[], Iterator] | None' = None, perturbation: 'float | np.ndarray | Callable[[], Iterator] | None' = None, last_avg: 'int' = 1, resamplings: 'int | dict[int, int]' = 1, perturbation_dims: 'int | None' = None, second_order: 'bool' = False, regularization: 'float | None' = None, hessian_delay: 'int' = 0, lse_solver: 'Callable[[np.ndarray, np.ndarray], np.ndarray] | None' = None, initial_hessian: 'np.ndarray | None' = None, callback: 'CALLBACK | None' = None, termination_checker: 'TERMINATIONCHECKER | None' = None) -> 'None'\n",
      "     |      Args:\n",
      "     |          maxiter: The maximum number of iterations. Note that this is not the maximal number\n",
      "     |              of function evaluations.\n",
      "     |          blocking: If True, only accepts updates that improve the loss (up to some allowed\n",
      "     |              increase, see next argument).\n",
      "     |          allowed_increase: If ``blocking`` is ``True``, this argument determines by how much\n",
      "     |              the loss can increase with the proposed parameters and still be accepted.\n",
      "     |              If ``None``, the allowed increases is calibrated automatically to be twice the\n",
      "     |              approximated standard deviation of the loss function.\n",
      "     |          trust_region: If ``True``, restricts the norm of the update step to be :math:`\\leq 1`.\n",
      "     |          learning_rate: The update step is the learning rate is multiplied with the gradient.\n",
      "     |              If the learning rate is a float, it remains constant over the course of the\n",
      "     |              optimization. If a NumPy array, the :math:`i`-th element is the learning rate for\n",
      "     |              the :math:`i`-th iteration. It can also be a callable returning an iterator which\n",
      "     |              yields the learning rates for each optimization step.\n",
      "     |              If ``learning_rate`` is set ``perturbation`` must also be provided.\n",
      "     |          perturbation: Specifies the magnitude of the perturbation for the finite difference\n",
      "     |              approximation of the gradients. See ``learning_rate`` for the supported types.\n",
      "     |              If ``perturbation`` is set ``learning_rate`` must also be provided.\n",
      "     |          last_avg: Return the average of the ``last_avg`` parameters instead of just the\n",
      "     |              last parameter values.\n",
      "     |          resamplings: The number of times the gradient (and Hessian) is sampled using a random\n",
      "     |              direction to construct a gradient estimate. Per default the gradient is estimated\n",
      "     |              using only one random direction. If an integer, all iterations use the same number\n",
      "     |              of resamplings. If a dictionary, this is interpreted as\n",
      "     |              ``{iteration: number of resamplings per iteration}``.\n",
      "     |          perturbation_dims: The number of perturbed dimensions. Per default, all dimensions\n",
      "     |              are perturbed, but a smaller, fixed number can be perturbed. If set, the perturbed\n",
      "     |              dimensions are chosen uniformly at random.\n",
      "     |          second_order: If True, use 2-SPSA instead of SPSA. In 2-SPSA, the Hessian is estimated\n",
      "     |              additionally to the gradient, and the gradient is preconditioned with the inverse\n",
      "     |              of the Hessian to improve convergence.\n",
      "     |          regularization: To ensure the preconditioner is symmetric and positive definite, the\n",
      "     |              identity times a small coefficient is added to it. This generator yields that\n",
      "     |              coefficient.\n",
      "     |          hessian_delay: Start multiplying the gradient with the inverse Hessian only after a\n",
      "     |              certain number of iterations. The Hessian is still evaluated and therefore this\n",
      "     |              argument can be useful to first get a stable average over the last iterations before\n",
      "     |              using it as preconditioner.\n",
      "     |          lse_solver: The method to solve for the inverse of the Hessian. Per default an\n",
      "     |              exact LSE solver is used, but can e.g. be overwritten by a minimization routine.\n",
      "     |          initial_hessian: The initial guess for the Hessian. By default the identity matrix\n",
      "     |              is used.\n",
      "     |          callback: A callback function passed information in each iteration step. The\n",
      "     |              information is, in this order: the number of function evaluations, the parameters,\n",
      "     |              the function value, the stepsize, whether the step was accepted.\n",
      "     |          termination_checker: A callback function executed at the end of each iteration step. The\n",
      "     |              arguments are, in this order: the parameters, the function value, the number\n",
      "     |              of function evaluations, the stepsize, whether the step was accepted. If the callback\n",
      "     |              returns True, the optimization is terminated.\n",
      "     |              To prevent additional evaluations of the objective method, if the objective has not yet\n",
      "     |              been evaluated, the objective is estimated by taking the mean of the objective\n",
      "     |              evaluations used in the estimate of the gradient.\n",
      "     |      \n",
      "     |      \n",
      "     |      Raises:\n",
      "     |          ValueError: If ``learning_rate`` or ``perturbation`` is an array with less elements\n",
      "     |              than the number of iterations.\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Get the support level dictionary.\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods defined here:\n",
      "     |  \n",
      "     |  calibrate(loss: 'Callable[[np.ndarray], float]', initial_point: 'np.ndarray', c: 'float' = 0.2, stability_constant: 'float' = 0, target_magnitude: 'float | None' = None, alpha: 'float' = 0.602, gamma: 'float' = 0.101, modelspace: 'bool' = False, max_evals_grouped: 'int' = 1) -> 'tuple[Callable, Callable]'\n",
      "     |      Calibrate SPSA parameters with a power series as learning rate and perturbation coeffs.\n",
      "     |      \n",
      "     |      The power series are:\n",
      "     |      \n",
      "     |      .. math::\n",
      "     |      \n",
      "     |          a_k = \\frac{a}{(A + k + 1)^\\alpha}, c_k = \\frac{c}{(k + 1)^\\gamma}\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          loss: The loss function.\n",
      "     |          initial_point: The initial guess of the iteration.\n",
      "     |          c: The initial perturbation magnitude.\n",
      "     |          stability_constant: The value of `A`.\n",
      "     |          target_magnitude: The target magnitude for the first update step, defaults to\n",
      "     |              :math:`2\\pi / 10`.\n",
      "     |          alpha: The exponent of the learning rate power series.\n",
      "     |          gamma: The exponent of the perturbation power series.\n",
      "     |          modelspace: Whether the target magnitude is the difference of parameter values\n",
      "     |              or function values (= model space).\n",
      "     |          max_evals_grouped: The number of grouped evaluations supported by the loss function.\n",
      "     |              Defaults to 1, i.e. no grouping.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          tuple(generator, generator): A tuple of power series generators, the first one for the\n",
      "     |              learning rate and the second one for the perturbation.\n",
      "     |  \n",
      "     |  estimate_stddev(loss: 'Callable[[np.ndarray], float]', initial_point: 'np.ndarray', avg: 'int' = 25, max_evals_grouped: 'int' = 1) -> 'float'\n",
      "     |      Estimate the standard deviation of the loss function.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SciPyOptimizer(qiskit_algorithms.optimizers.optimizer.Optimizer)\n",
      "     |  SciPyOptimizer(method: 'str | Callable', options: 'dict[str, Any] | None' = None, max_evals_grouped: 'int' = 1, **kwargs)\n",
      "     |  \n",
      "     |  A general Qiskit Optimizer wrapping scipy.optimize.minimize.\n",
      "     |  \n",
      "     |  For further detail, please refer to\n",
      "     |  https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SciPyOptimizer\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, method: 'str | Callable', options: 'dict[str, Any] | None' = None, max_evals_grouped: 'int' = 1, **kwargs)\n",
      "     |      Args:\n",
      "     |          method: Type of solver.\n",
      "     |          options: A dictionary of solver options.\n",
      "     |          kwargs: additional kwargs for scipy.optimize.minimize.\n",
      "     |          max_evals_grouped: Max number of default gradient evaluations performed simultaneously.\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Return support level dictionary\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class SteppableOptimizer(qiskit_algorithms.optimizers.optimizer.Optimizer)\n",
      "     |  SteppableOptimizer(maxiter: 'int' = 100)\n",
      "     |  \n",
      "     |  Base class for a steppable optimizer.\n",
      "     |  \n",
      "     |  This family of optimizers uses the `ask and tell interface\n",
      "     |  <https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/009_ask_and_tell.html>`_.\n",
      "     |  When using this interface the user has to call :meth:`~.ask` to get information about\n",
      "     |  how to evaluate the function (we are asking the optimizer about how to do the evaluation).\n",
      "     |  This information is typically the next points at which the function is evaluated, but depending\n",
      "     |  on the optimizer it can also determine whether to evaluate the function or its gradient.\n",
      "     |  Once the function has been evaluated, the user calls the method :meth:`~..tell`\n",
      "     |  to tell the optimizer what the result of the function evaluation(s) is. The optimizer then\n",
      "     |  updates its state accordingly and the user can decide whether to stop the optimization process\n",
      "     |  or to repeat a step.\n",
      "     |  \n",
      "     |  This interface is more customizable, and allows the user to have full control over the evaluation\n",
      "     |  of the function.\n",
      "     |  \n",
      "     |  Examples:\n",
      "     |  \n",
      "     |      An example where the evaluation of the function has a chance of failing. The user, with\n",
      "     |      specific knowledge about his function can catch this errors and handle them before passing\n",
      "     |      the result to the optimizer.\n",
      "     |  \n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          import random\n",
      "     |          import numpy as np\n",
      "     |          from qiskit_algorithms.optimizers import GradientDescent\n",
      "     |  \n",
      "     |          def objective(x):\n",
      "     |              if random.choice([True, False]):\n",
      "     |                  return None\n",
      "     |              else:\n",
      "     |                  return (np.linalg.norm(x) - 1) ** 2\n",
      "     |  \n",
      "     |          def grad(x):\n",
      "     |              if random.choice([True, False]):\n",
      "     |                  return None\n",
      "     |              else:\n",
      "     |                  return 2 * (np.linalg.norm(x) - 1) * x / np.linalg.norm(x)\n",
      "     |  \n",
      "     |  \n",
      "     |          initial_point = np.random.normal(0, 1, size=(100,))\n",
      "     |  \n",
      "     |          optimizer = GradientDescent(maxiter=20)\n",
      "     |          optimizer.start(x0=initial_point, fun=objective, jac=grad)\n",
      "     |  \n",
      "     |          while optimizer.continue_condition():\n",
      "     |              ask_data = optimizer.ask()\n",
      "     |              evaluated_gradient = None\n",
      "     |  \n",
      "     |              while evaluated_gradient is None:\n",
      "     |                  evaluated_gradient = grad(ask_data.x_center)\n",
      "     |                  optimizer.state.njev += 1\n",
      "     |  \n",
      "     |              optimizer.state.nit += 1\n",
      "     |  \n",
      "     |               cf  = TellData(eval_jac=evaluated_gradient)\n",
      "     |              optimizer.tell(ask_data=ask_data, tell_data=tell_data)\n",
      "     |  \n",
      "     |          result = optimizer.create_result()\n",
      "     |  \n",
      "     |  \n",
      "     |  Users that aren't dealing with complicated functions and who are more familiar with step by step\n",
      "     |  optimization algorithms can use the :meth:`~.step` method which wraps the :meth:`~.ask`\n",
      "     |  and :meth:`~.tell` methods. In the same spirit the method :meth:`~.minimize` will optimize the\n",
      "     |  function and return the result.\n",
      "     |  \n",
      "     |  To see other libraries that use this interface one can visit:\n",
      "     |  https://optuna.readthedocs.io/en/stable/tutorial/20_recipes/009_ask_and_tell.html\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      SteppableOptimizer\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, maxiter: 'int' = 100)\n",
      "     |      Args:\n",
      "     |          maxiter: Number of steps in the optimization process before ending the loop.\n",
      "     |  \n",
      "     |  ask(self) -> 'AskData'\n",
      "     |      Ask the optimizer for a set of points to evaluate.\n",
      "     |      \n",
      "     |      This method asks the optimizer which are the next points to evaluate.\n",
      "     |      These points can, e.g., correspond to function values and/or its derivative.\n",
      "     |      It may also correspond to variables that let the user infer which points to evaluate.\n",
      "     |      It is the first method inside of a :meth:`~.step` in the optimization process.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          An object containing the data needed to make the function evaluation to advance the\n",
      "     |          optimization process.\n",
      "     |  \n",
      "     |  continue_condition(self) -> 'bool'\n",
      "     |      Condition that indicates the optimization process should continue.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          ``True`` if the optimization process should continue, ``False`` otherwise.\n",
      "     |  \n",
      "     |  create_result(self) -> 'OptimizerResult'\n",
      "     |      Returns the result of the optimization.\n",
      "     |      \n",
      "     |      All the information needed to create such a result should be stored in the optimizer state\n",
      "     |      and will typically contain the best point found, the function value and gradient at that point,\n",
      "     |      the number of function and gradient evaluation and the number of iterations in the optimization.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization process.\n",
      "     |  \n",
      "     |  evaluate(self, ask_data: 'AskData') -> 'TellData'\n",
      "     |      Evaluates the function according to the instructions contained in :attr:`~.ask_data`.\n",
      "     |      \n",
      "     |      If the user decides to use :meth:`~.step` instead of :meth:`~.ask` and :meth:`~.tell`\n",
      "     |      this function will contain the logic on how to evaluate the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          ask_data: Contains the information on how to do the evaluation.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Data of all relevant information about the function evaluation.\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimizes the function.\n",
      "     |      \n",
      "     |      For well behaved functions the user can call this method to minimize a function.\n",
      "     |      If the user wants more control on how to evaluate the function a custom loop can be\n",
      "     |      created using :meth:`~.ask` and :meth:`~.tell` and evaluating the function manually.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: Function to minimize.\n",
      "     |          x0: Initial point.\n",
      "     |          jac: Function to compute the gradient.\n",
      "     |          bounds: Bounds of the search space.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          Object containing the result of the optimization.\n",
      "     |  \n",
      "     |  start(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'None'\n",
      "     |      Populates the state of the optimizer with the data provided and sets all the counters to 0.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: Function to minimize.\n",
      "     |          x0: Initial point.\n",
      "     |          jac: Function to compute the gradient.\n",
      "     |          bounds: Bounds of the search space.\n",
      "     |  \n",
      "     |  step(self) -> 'None'\n",
      "     |      Performs one step in the optimization process.\n",
      "     |      \n",
      "     |      This method composes :meth:`~.ask`, :meth:`~.evaluate`, and :meth:`~.tell` to make a \"step\"\n",
      "     |      in the optimization process.\n",
      "     |  \n",
      "     |  tell(self, ask_data: 'AskData', tell_data: 'TellData') -> 'None'\n",
      "     |      Updates the optimization state using the results of the function evaluation.\n",
      "     |      \n",
      "     |      A canonical optimization example using :meth:`~.ask` and :meth:`~.tell` can be seen\n",
      "     |      in :meth:`~.step`.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          ask_data: Contains the information on how the evaluation was done.\n",
      "     |          tell_data: Contains all relevant information about the evaluation of the objective\n",
      "     |              function.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  state\n",
      "     |      Return the current state of the optimizer.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'create_result', 'evaluate', 'get_sup...\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Return support level dictionary\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TNC(qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer)\n",
      "     |  TNC(maxiter: 'int' = 100, disp: 'bool' = False, accuracy: 'float' = 0, ftol: 'float' = -1, xtol: 'float' = -1, gtol: 'float' = -1, tol: 'float | None' = None, eps: 'float' = 1e-08, options: 'dict | None' = None, max_evals_grouped: 'int' = 1, **kwargs) -> 'None'\n",
      "     |  \n",
      "     |  Truncated Newton (TNC) optimizer.\n",
      "     |  \n",
      "     |  TNC uses a truncated Newton algorithm to minimize a function with variables subject to bounds.\n",
      "     |  This algorithm uses gradient information; it is also called Newton Conjugate-Gradient.\n",
      "     |  It differs from the :class:`CG` method as it wraps a C implementation and allows each variable\n",
      "     |  to be given upper and lower bounds.\n",
      "     |  \n",
      "     |  Uses scipy.optimize.minimize TNC\n",
      "     |  For further detail, please refer to\n",
      "     |  See https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TNC\n",
      "     |      qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, maxiter: 'int' = 100, disp: 'bool' = False, accuracy: 'float' = 0, ftol: 'float' = -1, xtol: 'float' = -1, gtol: 'float' = -1, tol: 'float | None' = None, eps: 'float' = 1e-08, options: 'dict | None' = None, max_evals_grouped: 'int' = 1, **kwargs) -> 'None'\n",
      "     |      Args:\n",
      "     |          maxiter: Maximum number of function evaluation.\n",
      "     |          disp: Set to True to print convergence messages.\n",
      "     |          accuracy: Relative precision for finite difference calculations.\n",
      "     |              If <= machine_precision, set to sqrt(machine_precision). Defaults to 0.\n",
      "     |          ftol: Precision goal for the value of f in the stopping criterion.\n",
      "     |              If ftol < 0.0, ftol is set to 0.0 defaults to -1.\n",
      "     |          xtol: Precision goal for the value of x in the stopping criterion\n",
      "     |              (after applying x scaling factors).\n",
      "     |              If xtol < 0.0, xtol is set to sqrt(machine_precision). Defaults to -1.\n",
      "     |          gtol: Precision goal for the value of the projected gradient in\n",
      "     |              the stopping criterion (after applying x scaling factors).\n",
      "     |              If gtol < 0.0, gtol is set to 1e-2 * sqrt(accuracy).\n",
      "     |              Setting it to 0.0 is not recommended. Defaults to -1.\n",
      "     |          tol: Tolerance for termination.\n",
      "     |          eps: Step size used for numerical approximation of the Jacobian.\n",
      "     |          options: A dictionary of solver options.\n",
      "     |          max_evals_grouped: Max number of default gradient evaluations performed simultaneously.\n",
      "     |          kwargs: additional kwargs for scipy.optimize.minimize.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer:\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Return support level dictionary\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.scipy_optimizer.SciPyOptimizer:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TellData(abc.ABC)\n",
      "     |  TellData(eval_fun: 'float | list[float] | None' = None, eval_jac: 'POINT | list[POINT] | None' = None) -> None\n",
      "     |  \n",
      "     |  Base class for argument type of :meth:`~.SteppableOptimizer.tell`.\n",
      "     |  \n",
      "     |  Args:\n",
      "     |      eval_fun: Image of the function at :attr:`~.ask_data.x_fun`.\n",
      "     |      eval_jac: Image of the gradient-jacobian at :attr:`~.ask_data.x_jac`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TellData\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __init__(self, eval_fun: 'float | list[float] | None' = None, eval_jac: 'POINT | list[POINT] | None' = None) -> None\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {'eval_fun': 'float | list[float] | None', 'eval_jac...\n",
      "     |  \n",
      "     |  __dataclass_fields__ = {'eval_fun': Field(name='eval_fun',type='float ...\n",
      "     |  \n",
      "     |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __match_args__ = ('eval_fun', 'eval_jac')\n",
      "     |  \n",
      "     |  eval_fun = None\n",
      "     |  \n",
      "     |  eval_jac = None\n",
      "    \n",
      "    class UMDA(qiskit_algorithms.optimizers.optimizer.Optimizer)\n",
      "     |  UMDA(maxiter: 'int' = 100, size_gen: 'int' = 20, alpha: 'float' = 0.5, callback: 'Callable[[int, np.array, float], None] | None' = None) -> 'None'\n",
      "     |  \n",
      "     |  Continuous Univariate Marginal Distribution Algorithm (UMDA).\n",
      "     |  \n",
      "     |  UMDA [1] is a specific type of Estimation of Distribution Algorithm (EDA) where new individuals\n",
      "     |  are sampled from univariate normal distributions and are updated in each iteration of the\n",
      "     |  algorithm by the best individuals found in the previous iteration.\n",
      "     |  \n",
      "     |  .. seealso::\n",
      "     |  \n",
      "     |      This original implementation of the UDMA optimizer for Qiskit was inspired by my\n",
      "     |      (Vicente P. Soloviev) work on the EDAspy Python package [2].\n",
      "     |  \n",
      "     |  EDAs are stochastic search algorithms and belong to the family of the evolutionary algorithms.\n",
      "     |  The main difference is that EDAs have a probabilistic model which is updated in each iteration\n",
      "     |  from the best individuals of previous generations (elite selection). Depending on the complexity\n",
      "     |  of the probabilistic model, EDAs can be classified in different ways. In this case, UMDA is a\n",
      "     |  univariate EDA as the embedded probabilistic model is univariate.\n",
      "     |  \n",
      "     |  UMDA has been compared to some of the already implemented algorithms in Qiskit library to\n",
      "     |  optimize the parameters of variational algorithms such as QAOA or VQE and competitive results\n",
      "     |  have been obtained [1]. UMDA seems to provide very good solutions for those circuits in which\n",
      "     |  the number of layers is not big.\n",
      "     |  \n",
      "     |  The optimization process can be personalized depending on the parameters chosen in the\n",
      "     |  initialization. The main parameter is the population size. The bigger it is, the final result\n",
      "     |  will be better. However, this increases the complexity of the algorithm and the runtime will\n",
      "     |  be much heavier. In the work [1] different experiments have been performed where population\n",
      "     |  size has been set to 20 - 30.\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      The UMDA implementation has more parameters but these have default values for the\n",
      "     |      initialization for better understanding of the user. For example, ``\u0007lpha`` parameter has\n",
      "     |      been set to 0.5 and is the percentage of the population which is selected in each iteration\n",
      "     |      to update the probabilistic model.\n",
      "     |  \n",
      "     |  \n",
      "     |  Example:\n",
      "     |  \n",
      "     |      This short example runs UMDA to optimize the parameters of a variational algorithm. Here we\n",
      "     |      will use the same operator as used in the algorithms introduction, which was originally\n",
      "     |      computed by Qiskit Nature for an H2 molecule. The minimum energy of the H2 Hamiltonian can\n",
      "     |      be found quite easily so we are able to set maxiters to a small value.\n",
      "     |  \n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          from qiskit_algorithms.optimizers import UMDA\n",
      "     |          from qiskit_algorithms import QAOA\n",
      "     |          from qiskit.quantum_info import Pauli\n",
      "     |          from qiskit.primitives import Sampler\n",
      "     |  \n",
      "     |          X = Pauli(\"X\")\n",
      "     |          I = Pauli(\"I\")\n",
      "     |          Z = Pauli(\"Z\")\n",
      "     |  \n",
      "     |          H2_op = (-1.052373245772859 * I ^ I) +             (0.39793742484318045 * I ^ Z) +             (-0.39793742484318045 * Z ^ I) +             (-0.01128010425623538 * Z ^ Z) +             (0.18093119978423156 * X ^ X)\n",
      "     |  \n",
      "     |          p = 2  # Toy example: 2 layers with 2 parameters in each layer: 4 variables\n",
      "     |  \n",
      "     |          opt = UMDA(maxiter=100, size_gen=20)\n",
      "     |          qaoa = QAOA(Sampler(), opt,reps=p)\n",
      "     |          result = qaoa.compute_minimum_eigenvalue(operator=H2_op)\n",
      "     |  \n",
      "     |      If it is desired to modify the percentage of individuals considered to update the\n",
      "     |      probabilistic model, then this code can be used. Here for example we set the 60% instead\n",
      "     |      of the 50% predefined.\n",
      "     |  \n",
      "     |      .. code-block:: python\n",
      "     |  \n",
      "     |          opt = UMDA(maxiter=100, size_gen=20, alpha = 0.6)\n",
      "     |          qaoa = QAOA(Sampler(), opt,reps=p)\n",
      "     |          result = qaoa.compute_minimum_eigenvalue(operator=H2_op)\n",
      "     |  \n",
      "     |  .. note::\n",
      "     |  \n",
      "     |      This component has some function that is normally random. If you want to reproduce behavior\n",
      "     |      then you should set the random number generator seed in the algorithm_globals\n",
      "     |      (``qiskit_algorithms.utils.algorithm_globals.random_seed = seed``).\n",
      "     |  \n",
      "     |  References:\n",
      "     |  \n",
      "     |      [1]: Vicente P. Soloviev, Pedro Larrañaga and Concha Bielza (2022, July). Quantum Parametric\n",
      "     |      Circuit Optimization with Estimation of Distribution Algorithms. In 2022 The Genetic and\n",
      "     |      Evolutionary Computation Conference (GECCO). DOI: https://doi.org/10.1145/3520304.3533963\n",
      "     |  \n",
      "     |      [2]: Vicente P. Soloviev. Python package EDAspy.\n",
      "     |      https://github.com/VicentePerezSoloviev/EDAspy.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      UMDA\n",
      "     |      qiskit_algorithms.optimizers.optimizer.Optimizer\n",
      "     |      abc.ABC\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, maxiter: 'int' = 100, size_gen: 'int' = 20, alpha: 'float' = 0.5, callback: 'Callable[[int, np.array, float], None] | None' = None) -> 'None'\n",
      "     |      Args:\n",
      "     |          maxiter: Maximum number of iterations.\n",
      "     |          size_gen: Population size of each generation.\n",
      "     |          alpha: Percentage (0, 1] of the population to be selected as elite selection.\n",
      "     |          callback: A callback function passed information in each iteration step. The\n",
      "     |              information is, in this order: the number of function evaluations, the parameters,\n",
      "     |              the best function value in this iteration.\n",
      "     |  \n",
      "     |  get_support_level(self)\n",
      "     |      Get the support level dictionary.\n",
      "     |  \n",
      "     |  minimize(self, fun: 'Callable[[POINT], float]', x0: 'POINT', jac: 'Callable[[POINT], POINT] | None' = None, bounds: 'list[tuple[float, float]] | None' = None) -> 'OptimizerResult'\n",
      "     |      Minimize the scalar function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          fun: The scalar function to minimize.\n",
      "     |          x0: The initial point for the minimization.\n",
      "     |          jac: The gradient of the scalar function ``fun``.\n",
      "     |          bounds: Bounds for the variables of ``fun``. This argument might be ignored if the\n",
      "     |              optimizer does not support bounds.\n",
      "     |      \n",
      "     |      Returns:\n",
      "     |          The result of the optimization, containing e.g. the result as attribute ``x``.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties defined here:\n",
      "     |  \n",
      "     |  settings\n",
      "     |      The optimizer settings in a dictionary format.\n",
      "     |      \n",
      "     |      The settings can for instance be used for JSON-serialization (if all settings are\n",
      "     |      serializable, which e.g. doesn't hold per default for callables), such that the\n",
      "     |      optimizer object can be reconstructed as\n",
      "     |      \n",
      "     |      .. code-block::\n",
      "     |      \n",
      "     |          settings = optimizer.settings\n",
      "     |          # JSON serialize and send to another server\n",
      "     |          optimizer = OptimizerClass(**settings)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  alpha\n",
      "     |      Returns the alpha parameter value (percentage of population selected to update\n",
      "     |      probabilistic model)\n",
      "     |  \n",
      "     |  maxiter\n",
      "     |      Returns the maximum number of iterations\n",
      "     |  \n",
      "     |  size_gen\n",
      "     |      Returns the size of the generations (number of individuals per generation)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  ELITE_FACTOR = 0.4\n",
      "     |  \n",
      "     |  STD_BOUND = 0.3\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  __annotations__ = {}\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  print_options(self)\n",
      "     |      Print algorithm-specific options.\n",
      "     |  \n",
      "     |  set_max_evals_grouped(self, limit)\n",
      "     |      Set max evals grouped\n",
      "     |  \n",
      "     |  set_options(self, **kwargs)\n",
      "     |      Sets or updates values in the options dictionary.\n",
      "     |      \n",
      "     |      The options dictionary may be used internally by a given optimizer to\n",
      "     |      pass additional optional values for the underlying optimizer/optimization\n",
      "     |      function used. The options dictionary may be initially populated with\n",
      "     |      a set of key/values when the given optimizer is constructed.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          kwargs (dict): options, given as name=value.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Static methods inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  gradient_num_diff(x_center, f, epsilon, max_evals_grouped=None)\n",
      "     |      We compute the gradient with the numeric differentiation in the parallel way,\n",
      "     |      around the point x_center.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          x_center (ndarray): point around which we compute the gradient\n",
      "     |          f (func): the function of which the gradient is to be computed.\n",
      "     |          epsilon (float): the epsilon used in the numeric differentiation.\n",
      "     |          max_evals_grouped (int): max evals grouped, defaults to 1 (i.e. no batching).\n",
      "     |      Returns:\n",
      "     |          grad: the gradient computed\n",
      "     |  \n",
      "     |  wrap_function(function, args)\n",
      "     |      Wrap the function to implicitly inject the args at the call of the function.\n",
      "     |      \n",
      "     |      Args:\n",
      "     |          function (func): the target function\n",
      "     |          args (tuple): the args to be injected\n",
      "     |      Returns:\n",
      "     |          function_wrapper: wrapper\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Readonly properties inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  bounds_support_level\n",
      "     |      Returns bounds support level\n",
      "     |  \n",
      "     |  gradient_support_level\n",
      "     |      Returns gradient support level\n",
      "     |  \n",
      "     |  initial_point_support_level\n",
      "     |      Returns initial point support level\n",
      "     |  \n",
      "     |  is_bounds_ignored\n",
      "     |      Returns is bounds ignored\n",
      "     |  \n",
      "     |  is_bounds_required\n",
      "     |      Returns is bounds required\n",
      "     |  \n",
      "     |  is_bounds_supported\n",
      "     |      Returns is bounds supported\n",
      "     |  \n",
      "     |  is_gradient_ignored\n",
      "     |      Returns is gradient ignored\n",
      "     |  \n",
      "     |  is_gradient_required\n",
      "     |      Returns is gradient required\n",
      "     |  \n",
      "     |  is_gradient_supported\n",
      "     |      Returns is gradient supported\n",
      "     |  \n",
      "     |  is_initial_point_ignored\n",
      "     |      Returns is initial point ignored\n",
      "     |  \n",
      "     |  is_initial_point_required\n",
      "     |      Returns is initial point required\n",
      "     |  \n",
      "     |  is_initial_point_supported\n",
      "     |      Returns is initial point supported\n",
      "     |  \n",
      "     |  setting\n",
      "     |      Return setting\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from qiskit_algorithms.optimizers.optimizer.Optimizer:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Optimizer', 'OptimizerSupportLevel', 'SteppableOptimizer',...\n",
      "\n",
      "FILE\n",
      "    c:\\users\\parvez-pc\\anaconda3\\lib\\site-packages\\qiskit_algorithms\\optimizers\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import qiskit\n",
    "help('qiskit_algorithms.optimizers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278e4db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "QML_Reg(Dict, Features, Target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d215400a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, SGDRegressor, ElasticNet\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "87f341ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8806054401686505"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GradientBoostingRegressor()\n",
    "Features = MinMaxScaler().fit_transform(Features)\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "        Features, Target, train_size=0.7)\n",
    "model.fit(train_features, train_labels)\n",
    "model.score(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a39bae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10103415.805656    9842086.6168797  10277160.81898271 10103415.805656  ]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict([[6231, 3, 2, 2, 0, 1, 0, 1, 1, 1, 0, 1], [9256, 4, 3, 2, 1, 1, 1, 0, 0, 0, 0, 0], [4006, 2, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1], [7150, 4, 2, 3, 0, 1, 0, 1, 1, 2, 0, 2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "185b53b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "values=[]\n",
    "for i in range(len(test_features)):\n",
    "        values.append(i*10)\n",
    "\n",
    "pred_test_features = model.predict(test_features)       \n",
    "plt.plot(values, test_labels, \"bo\") \n",
    "plt.plot(values, pred_test_features, \"go\") \n",
    "    \n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
